{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d953a1-cadf-4b12-848e-aec44b1f9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229b6573-260c-4e1a-8032-4db2627a2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b27301a8-9e76-4b7d-a59c-ba3fa44b91d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a58866-97da-4668-8a27-46da39623d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40, output_scores=True, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b961f9f-65d2-4ef4-a3d4-6031b899602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202cb2ab-2770-4884-8e32-36f48894a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1507f42-d8fa-4c2c-8996-1bf153d31081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function choice:\n",
      "\n",
      "choice(...) method of numpy.random.mtrand.RandomState instance\n",
      "    choice(a, size=None, replace=True, p=None)\n",
      "\n",
      "    Generates a random sample from a given 1-D array\n",
      "\n",
      "    .. versionadded:: 1.7.0\n",
      "\n",
      "    .. note::\n",
      "        New code should use the `~numpy.random.Generator.choice`\n",
      "        method of a `~numpy.random.Generator` instance instead;\n",
      "        please see the :ref:`random-quick-start`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : 1-D array-like or int\n",
      "        If an ndarray, a random sample is generated from its elements.\n",
      "        If an int, the random sample is generated as if it were ``np.arange(a)``\n",
      "    size : int or tuple of ints, optional\n",
      "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
      "        ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
      "        single value is returned.\n",
      "    replace : boolean, optional\n",
      "        Whether the sample is with or without replacement. Default is True,\n",
      "        meaning that a value of ``a`` can be selected multiple times.\n",
      "    p : 1-D array-like, optional\n",
      "        The probabilities associated with each entry in a.\n",
      "        If not given, the sample assumes a uniform distribution over all\n",
      "        entries in ``a``.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    samples : single item or ndarray\n",
      "        The generated random samples\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If a is an int and less than zero, if a or p are not 1-dimensional,\n",
      "        if a is an array-like of size 0, if p is not a vector of\n",
      "        probabilities, if a and p have different lengths, or if\n",
      "        replace=False and the sample size is greater than the population\n",
      "        size\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    randint, shuffle, permutation\n",
      "    random.Generator.choice: which should be used in new code\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    Setting user-specified probabilities through ``p`` uses a more general but less\n",
      "    efficient sampler than the default. The general sampler produces a different sample\n",
      "    than the optimized sampler even if each element of ``p`` is 1 / len(a).\n",
      "\n",
      "    Sampling random rows from a 2-D array is not possible with this function,\n",
      "    but is possible with `Generator.choice` through its ``axis`` keyword.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    Generate a uniform random sample from np.arange(5) of size 3:\n",
      "\n",
      "    >>> np.random.choice(5, 3)\n",
      "    array([0, 3, 4]) # random\n",
      "    >>> #This is equivalent to np.random.randint(0,5,3)\n",
      "\n",
      "    Generate a non-uniform random sample from np.arange(5) of size 3:\n",
      "\n",
      "    >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([3, 3, 0]) # random\n",
      "\n",
      "    Generate a uniform random sample from np.arange(5) of size 3 without\n",
      "    replacement:\n",
      "\n",
      "    >>> np.random.choice(5, 3, replace=False)\n",
      "    array([3,1,0]) # random\n",
      "    >>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n",
      "\n",
      "    Generate a non-uniform random sample from np.arange(5) of size\n",
      "    3 without replacement:\n",
      "\n",
      "    >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([2, 3, 0]) # random\n",
      "\n",
      "    Any of the above can be repeated with an arbitrary array-like\n",
      "    instead of just integers. For instance:\n",
      "\n",
      "    >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n",
      "    >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\n",
      "    array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n",
      "          dtype='<U11')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7476f4f2-5948-4b43-9eb1-cdf8fe3cf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(generate_func, input_ids, max_new_tokens) -> str:\n",
    "    for i in tqdm(range(max_new_tokens)):\n",
    "        logits = generate_func(\n",
    "            {\n",
    "                'input_ids':input_ids, \n",
    "                'attention_mask':torch.ones_like(input_ids).to(torch_device),\n",
    "                'pad_token_id':50256\n",
    "            }\n",
    "        )\n",
    "        next_token = logits.argmax(axis=1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(dim=1)], axis=1)\n",
    "    return tokenizer.batch_decode(input_ids)\n",
    "\n",
    "\n",
    "def sample(probs, top_p):\n",
    "    sorted_probs, sorted_indices = probs.sort(descending=True)\n",
    "    cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "    mask = cumsum > top_p\n",
    "    if mask.sum() == len(mask):\n",
    "        mask[0] = False\n",
    "    sorted_probs[mask] = 0\n",
    "    probs[sorted_indices] = sorted_probs\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    probs = probs / probs.sum()\n",
    "    next_token = np.random.choice(len(probs), p=probs)\n",
    "    return next_token\n",
    "    # input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(torch_device)], axis=1)\n",
    "    \n",
    "    \n",
    "def top_p_sampling(generate_func, input_ids, max_new_tokens, top_p) -> str:\n",
    "\n",
    "    '''\n",
    "    top_p = 0.95\n",
    "    probs = [[0.1, 0.7, 0.2]]\n",
    "    sorted_probs = [[0.7, 0.2, 0.1]]\n",
    "    sorted_indices = [[1, 2, 0]]\n",
    "    cumsum = [[0.7, 0.9, 1.0]]\n",
    "    mask = cumsum > top_p\n",
    "    mask = [[False, False, True]]\n",
    "    sorted_probs[mask] = 0\n",
    "    sorted_probs = [[0.7, 0.2, 0]]\n",
    "    !!!XXX  probs = sorted_probs[sorted_indices] XXX!!!\n",
    "    probs = [[0.0, 0.7, 0.2]]\n",
    "\n",
    "    probs[sorted_indices] = sorted_probs\n",
    "    '''\n",
    "    \n",
    "    for i in tqdm(range(max_new_tokens)):\n",
    "        probs = torch.softmax(generate_func(\n",
    "            {\n",
    "                'input_ids':input_ids, \n",
    "                'attention_mask':torch.ones_like(input_ids).to(torch_device),\n",
    "                'pad_token_id':50256\n",
    "            }\n",
    "        ), axis=1).squeeze()\n",
    "        next_token = sample(probs, top_p)\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token]]).to(torch_device)], axis=1)\n",
    "    return tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e89e3d-0385-4a54-8713-f29e3417522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60940c8d734b4e60b86a3b1de069a010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\\n\\nI'm not sure\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_decode(\n",
    "    lambda model_inputs: model.generate(\n",
    "        **model_inputs, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True, \n",
    "        max_new_tokens=1\n",
    "    ).scores[0],\n",
    "    input_ids=model_inputs.input_ids,\n",
    "    max_new_tokens=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9b08e1-d9ab-469c-bd3f-548b75d3b950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\\n\\nI'm not sure\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(greedy_output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7cbfec27-2bba-489f-b26f-ce26c40223d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcec8667f6d4becbe611f73723800c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['I enjoy walking with my cute dog,\" said Hicks. \"The crowds are huge.\\n\\n\"When I see him, the sky is black with people everywhere. I go back and forth between the security guards and the man there.\"']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_p_sampling(\n",
    "    lambda model_inputs: model.generate(\n",
    "        **model_inputs, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True, \n",
    "        max_new_tokens=1\n",
    "    ).scores[0],\n",
    "    input_ids=model_inputs.input_ids,\n",
    "    max_new_tokens=40,\n",
    "    top_p=0.92\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "040e1a95-72e5-41aa-b3d4-6cb566f99413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7000, 0.2000, 0.1000])\n",
      "tensor([1, 2, 0])\n",
      "tensor([0.7000, 0.9000, 1.0000])\n",
      "tensor([False, False,  True])\n",
      "tensor([0.2000, 0.0000, 0.7000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(torch.tensor([0.1, 0.7, 0.2]), 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc199bc-2ca1-4e9c-8bd2-bd4bb4778d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    top_p = 0.95\n",
    "    probs = [[0.1, 0.7, 0.2]]\n",
    "    sorted_probs = [[0.7, 0.2, 0.1]]\n",
    "    sorted_indices = [[1, 2, 0]]\n",
    "    cumsum = [[0.7, 0.9, 1.0]]\n",
    "    mask = cumsum > top_p\n",
    "    mask = [[False, False, True]]\n",
    "    sorted_probs[mask] = 0\n",
    "    sorted_probs = [[0.7, 0.2, 0]]\n",
    "    probs = sorted_probs[sorted_indices]\n",
    "    probs = [[0.0, 0.7, 0.2]]\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
