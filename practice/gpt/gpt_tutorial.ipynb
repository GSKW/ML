{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjtCcOdeAjEa"
   },
   "source": [
    "## Языковая Модель GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTSoipTY4rJe"
   },
   "source": [
    "Смотри лекцию по теме по ссылке: https://docs.google.com/presentation/d/1AcKnTMw0L6PZX9LK2JJkVIx5rgOSU2s6lL9DSfE6yNA/edit?usp=sharing\n",
    "\n",
    "- Что такое gpt-2\n",
    "  - архитектура\n",
    "  - входы выходы\n",
    "\n",
    "  <br></br>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=13ya5tFwrHpD7x9BBSjx3G0ojiMwvmR3L\" width=500>  \n",
    "<br></br>\n",
    "\n",
    "GPT-2 - это трансформер, который был обучен предсказывать следующий токен на огромном датасете текстов. В основе данной архитектуры лежит несколько transformer-блоков, каждый из которых передаёт свой выход на вход следующему блоку. \n",
    "\n",
    "В GPT используется необычный токенайзер, который называется BPE-tokenizer. Если слово есть в словаре токенайзера то оно превращается в один индекс, если его нет в словаре, то слово разбивается на составные части и снова ищется в словаре. Это происходит до тех пор, пока мы не найдём все составные части слова в словаре.\n",
    "\n",
    "Словарь GPT изначально заполняется всеми одиночными символами, а после дополняется наиболее частыми последовательностями символов.\n",
    "\n",
    "\n",
    "### Задача, на которой обучался GPT\n",
    "\n",
    "Как уже было сказано, GPT обучался на задаче предсказания следующих токенов по предыдущим, эта задача называется Language Modelling. \n",
    "\n",
    "<br></br>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1_oG5CLmFmCTlWO8te1KoAf8BTpvSxHX7\" width=500>  \n",
    "<br></br>\n",
    "\n",
    "А именно, мы максимизируем вероятность правильного токена при условии предыдущих токенов.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCRd0CAZIfKK"
   },
   "source": [
    "Мы будем работать с библиотекой `transformers` от HuggingFace и использовать её `torch` backend. Для загрузки данных возьмём библиотеку `datasets` от тех же разработчиков.  \n",
    "\n",
    "Устанвим их с помощью `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxbuUov0m5Q4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcBWviDlJfri"
   },
   "source": [
    "Импортируем всё что нам понадобится заранее, однако, пока нам нужны только `AutoTokenizer` и `AutoModelForCausalLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZc9Mh1vKlhd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, default_data_collator\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CPHpX_LJ-iN"
   },
   "source": [
    "Для наших экспериментов возьмём небольшую русскую модель GPT-3 от сбера: https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2\n",
    "\n",
    "Можно так же взять [среднюю](https://huggingface.co/sberbank-ai/rugpt3medium_based_on_gpt2) или [большую](https://huggingface.co/sberbank-ai/rugpt3large_based_on_gpt2) модель, однако, они будут заметно дольше считаться. \n",
    "\n",
    "Можно посмотреть и другие модели, например, GPT-2 от них же: https://huggingface.co/sberbank-ai.\n",
    "\n",
    "Для работы нам понадобится сама модель и токенизатор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pfuyLorKw2v"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO0sfc3x5OpK"
   },
   "source": [
    "GPT - огромные модели, размеры большого варианта модели приближаются к миллиарду параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_zaP4CH3yQn"
   },
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for p in model.parameters():\n",
    "    total_params += torch.prod(torch.tensor(p.shape))\n",
    "\n",
    "print(f'Количество параметров сети: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM71WhLOqJwh"
   },
   "source": [
    "Чтобы определить какой именно класс был использован для построения модели, что полезно для поиска документации, можно использовать встроенную функцию `type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI0him9VqHxh"
   },
   "outputs": [],
   "source": [
    "type(model), type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA2UZMSJrjGF"
   },
   "source": [
    "По этим классам легко найти [документацию](https://huggingface.co/docs/transformers/model_doc/gpt2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV0otTn85ktE"
   },
   "source": [
    "Попробуем разбить на токены какой-нибудь текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rtXAqJS5vl0"
   },
   "outputs": [],
   "source": [
    "text = 'Сосиска в тексте'\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeZo98VY59Jq"
   },
   "source": [
    "К сожалению, из-за того, что все символы, исходно представленные в кодировке `utf-8`, разбиты на байты BPE токенизатором, результ малочитаемый. Однако, что-то полезное уже можно увидеть из разбиения на токены, а именно, что слова было три, а токенов получилось 5. Видимо, слово \"Сосиска\" с большой буквы не так уж часто встречается в текстах, поэтому для него нет отдельного токена. Для частых слов, как правило, в словаре уже есть отдельный токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSoDQefCrhYU"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"клаустрофобия\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYlnCt4O0kvQ"
   },
   "source": [
    "Для некоторых моделей результат токенизации читаемый, однако, можно попробовать их превратить в читаемые методом `convert_tokens_to_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiE958gn07jW"
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(['ÐºÐ»Ð°', 'ÑĥÑģÑĤÑĢÐ¾'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBPkXfUvUMRX"
   },
   "source": [
    "**Задача 0**\n",
    "\n",
    "Найдите пару слов которые токенизируются в один токен и одно **существующее** слово, которое разбивается на наибольшее количество токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbOYzgr1Vfvs"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"это\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlJAL1zB7CLJ"
   },
   "source": [
    "Токенизация - это лишь часть подготовки текста для подачи в модель, за ней следует перевод токенов в индексы. Перевод в индексы осуществляется по словарю, который хранится в токенайзере. Словарь вида `{'a': 0, 'ab': 1, ...}`. По этим индексам модель возьмёт соответствующие вектора токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uhr_9MfrLGjN"
   },
   "outputs": [],
   "source": [
    "text = 'Сосиска в тексте'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xp7rEEF7rL1"
   },
   "source": [
    "Практически в таком виде модель принимает входные данные, остаётся только конвертировать индексы в тензор и добавить `batch` размерность. Всё это вместе можно сделать одним уже готовым методом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK-rAR5b77eC"
   },
   "outputs": [],
   "source": [
    "batch = tokenizer(text, return_tensors='pt')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Lxca0lrHlGO"
   },
   "source": [
    "Вместе с индексами токенов токенизатор вернул и маску для attention-а, в случае, когда текст один, она не несёт информативной нагрузки, однако, когда в batch-е встречаются тексты разной длины, она позволяет не смотреть на паддинги (дополнения неинформативным символом текстов до одной длины в токенах).\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Тензоры, полученные на выходе токенизатора, многомерные. Часто удобно посмотреть на их размерности чтобы понять что к чему. \n",
    "\n",
    "**Задача 1**\n",
    "\n",
    "Выведем эти размерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_DtBzf1Uinv"
   },
   "outputs": [],
   "source": [
    "print(batch['input_ids'].shape)\n",
    "\n",
    "# ВАШ КОД, который выводит размерность маски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoCMhthWMQ60"
   },
   "source": [
    "Перейдём к модели. Для начала, перенесём её на GPU, иначе считаться на CPU будет очень долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdxXfGmdUYk_"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu:0')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tKxsR-wD6FU"
   },
   "source": [
    "Перенесём полученный входной батч на GPU и подадим его в модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shQvSiaFOTt1"
   },
   "outputs": [],
   "source": [
    "batch_gpu = batch.to(device)\n",
    "outs = model(**batch_gpu)\n",
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPvhvaJ9ESl-"
   },
   "source": [
    "В outs лежит словарь с тензороми, посмотрим ключи словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkOz0Nx0Odtd"
   },
   "outputs": [],
   "source": [
    "outs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6wTJxt1EfQd"
   },
   "source": [
    "Нас интересует тензор `logits`, это логарифм вероятностей следующего токена.\n",
    "\n",
    "**Задача 2**\n",
    "\n",
    "Посмотрим на размерности этого тезора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x45Esud4Fj3W"
   },
   "outputs": [],
   "source": [
    "print(outs['logits'] # ВАШ КОД ЗДЕСЬ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ2g4FNKOILi"
   },
   "source": [
    "Обратите внимание, что для данной модели сначала идет размерность `batch_size`, а потом уже `max_seq_len`. Расскажите соседу, как взять вероятность токена который мы будем генерировать следующим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn7iMmP9HSPt"
   },
   "source": [
    "**Задача 2** * (можно пропустить)\n",
    "\n",
    "Вариант еще сложнее:\n",
    "\n",
    "- по лог-вероятностям вычислить вероятности (здесь нужен `torch.softmax(###, dim=seq_dim)`, где `seq_dim` - номер размерности, имеющей размер словаря).\n",
    "- выбрать последний токен в последовательности \n",
    "- засемплировать следующий токен с помощью примерно такого кода\n",
    "```python\n",
    "In [16]: torch.distributions.Categorical(torch.tensor([0.1, 0.2, 0.7])).sample()\n",
    "Out[16]: tensor(2)\n",
    "```\n",
    "вероятности подставить свои.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6YpuzYsGCln"
   },
   "source": [
    "Теперь попробуем сгенерировать что-нибудь, подав на вход наш текст `Сосиска в тексте`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNTapD4iLMC9"
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "do_sample = True\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "outputs = model.generate(**batch_gpu, \n",
    "                         return_dict_in_generate=True, # Это важный параметр!\n",
    "                         output_scores=True, # Это важный параметр!\n",
    "                         top_k=top_k,\n",
    "                         top_p=top_p,\n",
    "                         do_sample=do_sample, \n",
    "                         max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bamyhKS5L5u_"
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOprEmKYRqVT"
   },
   "source": [
    "**Задача 3**\n",
    "\n",
    "- задайте свой текст\n",
    "- токенизируйте его\n",
    "- поместите батч на GPU\n",
    "- сгенерируйте продолжение моделью\n",
    "- длина сгенерированного текста должна быть не меньше 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBwoHZ5-UfIb"
   },
   "source": [
    "### Дообучение модели\n",
    "\n",
    "Для дообучения модели нам нужны тексты. Для примера создадим маленький датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0slDw4uU2yu"
   },
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'w') as fp:\n",
    "    fp.write('Это первый текст да\\nэто второй текст')\n",
    "\n",
    "with open('dataset_test.txt', 'w') as fp:\n",
    "    fp.write('Это вовсе не текст\\nэто может быть второй текст')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsA9MVMZVIAm"
   },
   "source": [
    "Тут всего две строчки, будем считать что это два семпла (хотя могли бы взять обе строчки как один семпл). Каждый текст положим в отдельный словарь по ключу `input` и спарсим его в `json`-строчку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeB_adfShYkk"
   },
   "outputs": [],
   "source": [
    "with open('dataset.json', 'w') as fp:\n",
    "    with open('dataset.txt') as fpt:\n",
    "        texts = fpt.read().split('\\n')\n",
    "    for text in texts:\n",
    "        fp.write(json.dumps({'input': text})+ '\\n')\n",
    "\n",
    "with open('dataset_test.json', 'w') as fp:\n",
    "    with open('dataset_test.txt') as fpt:\n",
    "        texts = fpt.read().split('\\n')\n",
    "    for text in texts:\n",
    "        fp.write(json.dumps({'input': text}) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OfrDy4sWOZ3"
   },
   "source": [
    "Создадим датасет из строк из получившегося `json`-a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL7mkGuWhe4w"
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset('json', data_files={'train': 'dataset.json', 'test': 'dataset_test.json'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfX95cZSWYAt"
   },
   "source": [
    "Сделаем токенизацию датасета и добавим целевые переменные `labels`, они будут идентичны самими входам (библиотека `transformers` сама сдвинет последовательность для предсказания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjsQooeFOmij"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['input'], return_tensors='pt')\n",
    "\n",
    "\n",
    "def add_labels(example):\n",
    "    example['labels'] = example['input_ids']\n",
    "    return example\n",
    "\n",
    "\n",
    "columns_to_remove = raw_dataset[\"train\"].column_names\n",
    "ds = raw_dataset.map(tokenize_function, remove_columns=columns_to_remove, load_from_cache_file=False).map(add_labels, load_from_cache_file=False)\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7lCm182W-YD"
   },
   "source": [
    "Посмотрим на размеры полученных входных индексов токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wj_jwybXVFy"
   },
   "outputs": [],
   "source": [
    "ds['train'][0]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjl1jbehXOUt"
   },
   "source": [
    "**Задача 4**\n",
    "\n",
    "Выведите количество семплов в датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OUbsXiaXNK6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfSS9-SrXorO"
   },
   "source": [
    "Сделаем `DataLoader`, который будет генерировать батчи. Для этого сначала напишем collate функцию, которая дополняет до нужной длины нулями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo4zphNoXa4p"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.pad_token = tokenizer.get_vocab()\n",
    "\n",
    "\n",
    "dl = DataLoader(ds['train'], batch_size=1, collate_fn=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
    "dl_test = DataLoader(ds['test'], batch_size=1, collate_fn=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tWT2GGiZ_yH"
   },
   "source": [
    "Посмотрим что генерирует `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og37DOVwfXpS"
   },
   "outputs": [],
   "source": [
    "next(iter(dl_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9sVHjYppkYn"
   },
   "source": [
    "**Задача 4** *\n",
    "\n",
    "Настройте `DataCollatorForLanguageModeling` для батча > 1 и текстов разной длины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6BNMO8Bdl_d"
   },
   "source": [
    "Давайте проверим, что выдает текущий gpt для входа 'Это'. Мы ожидаем, что до обучения gpt будет продолжать как-то, а после обучения как 'Это наш первый вариант':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24oiLwFGbt8Z"
   },
   "outputs": [],
   "source": [
    "text = 'Это'\n",
    "\n",
    "max_length = 100\n",
    "do_sample = True\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "outputs = model.generate(**tokenizer(text, return_tensors='pt').to(device), \n",
    "                         return_dict_in_generate=True, # Это важный параметр!\n",
    "                         output_scores=True, # Это важный параметр!\n",
    "                         top_k=top_k,\n",
    "                         top_p=top_p,\n",
    "                         do_sample=do_sample, \n",
    "                         max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwkdMldyfrzi"
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(out.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6BNMO8Bdl_d"
   },
   "source": [
    "Теперь мы можем готовить батчи, осталось подготовить оптимизитор и запустить обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEAjCrtTdkyJ"
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41QrgOpkbcNY"
   },
   "outputs": [],
   "source": [
    "epochs = 32\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    total_loss = 0\n",
    "    for batch in dl:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(**batch.to(device))\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += output.loss.item()\n",
    "\n",
    "    # Оцениваем loss на тестовой выборке\n",
    "\n",
    "    # Экономим пямять чтобы не хранить информацию для градиентов\n",
    "    with torch.no_grad():\n",
    "        total_loss_test = 0\n",
    "        for batch in dl_test:\n",
    "            output = model(**batch.to(device))\n",
    "            total_loss_test += output.loss.item()\n",
    "  \n",
    "    print(f'Mean loss on epoch {epoch}: train {total_loss / len(dl):.3f} test {total_loss_test / len(dl_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24oiLwFGbt8Z"
   },
   "outputs": [],
   "source": [
    "text = 'Это'\n",
    "\n",
    "max_length = 100\n",
    "do_sample = True\n",
    "top_k = 50\n",
    "top_p = 0.95\n",
    "\n",
    "outputs = model.generate(**tokenizer(text, return_tensors='pt').to(device), \n",
    "                         return_dict_in_generate=True, # Это важный параметр!\n",
    "                         output_scores=True, # Это важный параметр!\n",
    "                         top_k=top_k,\n",
    "                         top_p=top_p,\n",
    "                         do_sample=do_sample, \n",
    "                         max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwkdMldyfrzi"
   },
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(out.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfFxxjX-hAB8"
   },
   "source": [
    "**Задача 5**\n",
    "\n",
    "Найдите свой набор текстов и замените игрушечную пару предложений на него в коде \n",
    "```python\n",
    "import json\n",
    "\n",
    "with open('dataset.txt', 'w') as fp:\n",
    "  fp.write('Это первый текст да\\nэто второй текст')\n",
    "\n",
    "with open('dataset.json', 'w') as fp:\n",
    "  with open('dataset.txt') as fpt:\n",
    "    texts = fpt.read().split('\\n')\n",
    "  for text in texts:\n",
    "    fp.write(json.dumps({'input': text}))\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jfR37L2jmEI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP+YL4CxJyYd2jRqGenEunU",
   "collapsed_sections": [],
   "mount_file_id": "1g4zhYPzhI6DxMpnKJfOEqBGtQtXLg6-y",
   "name": "GoTo: Generative_Models_for_Text.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
