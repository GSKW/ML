{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.datasets import load_boston\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация текстов -- начало\n",
    "\n",
    "Презентация про RNN доступна по ссылке: https://docs.google.com/presentation/d/1tEvg1ozFFX_YzVBhCTR18fhmdsX51W4QgTaGzWRvkEE/edit?usp=sharing\n",
    "\n",
    "Существуют блоки, созданные специально для обработки последовательностей, какой является текст. Можно сказать, что текст -- это последовательность символов, слов, подслов и так далее. Части, из которых состоит текст, называются *токенами*. А первым этапом при решении любой NLP задачи является токенизация.\n",
    "Как правило, разным токенам сопоставляются их id -- числа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1** Дан файл Names.txt -- содержащий 55 тысяч индийских имён. Научимся же генерировать на их основе новые!\n",
    "\n",
    "1) Прочитайте этот файлик и положите имена в список `all_names`\n",
    "2) Сделайте из него три списка имён -- train, val и test -- разбейте файл в соотношении 0.8, 0.1, и 0.1 соответственно. Для этого подходит функция `train_test_split`из `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2** Посчитайте два словарика -- `id2label` и `label2id`. Первый отображает айди токена (в данном случае токен = символ) в токен(символ). Второй словарик же просто обратен первому. Важно, что оба словаря должны быть построены на основе train, причём помимо символов оттуда должны содержаться следующие:\n",
    " - '$' -- этот символ будет обозначать начало последовательности, его id должен быть равен 2\n",
    " - '^' -- этот -- конец последовательности, его id должен быть равен 1\n",
    " - '_' -- этот -- отступ, позже будет объяснено зачем, его id должен быть равен 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_names = pd.read_csv('train_supervised_dataset.csv').fillna('')[['name', 'good', 'brand']].values.tolist()[:2000]\n",
    "all_names = [x[0] + ' <GD> '+ x[1] + ' <BR> ' + x[2] for x in all_names]\n",
    "test_names = pd.read_csv('test_dataset.csv').fillna('')['name'].values.tolist()\n",
    "add_names = pd.read_csv('train_unsupervised_dataset.csv').fillna('')['name'].values.tolist()\n",
    "\n",
    "symb = ''.join(all_names)\n",
    "test_symb = ''.join(test_names)\n",
    "add_symb = ''.join(add_names)\n",
    "un_symb = list(set([x for x in symb+test_symb+add_symb]))\n",
    "\n",
    "id2label = {\n",
    "    0:'<pad>',\n",
    "    1:'<eos>',\n",
    "    2:'<sos>',\n",
    "    3:'<BR>',\n",
    "    4:'<GD>'\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    '<pad>':0,\n",
    "    '<eos>':1,\n",
    "    '<sos>':2,\n",
    "    '<BR>':3,\n",
    "    '<GD>':4\n",
    "}\n",
    "\n",
    "ln = len(id2label)\n",
    "\n",
    "for x in range(len(un_symb)):\n",
    "    id2label[x+ln] = un_symb[x]\n",
    "    label2id[un_symb[x]] = x+ln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del add_names, test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(id2label) == len(label2id)\n",
    "# assert '_' in label2id\n",
    "# assert '$' in label2id\n",
    "# assert '^' in label2id\n",
    "# assert label2id['_'] == 0\n",
    "# assert label2id['$'] == 2\n",
    "# assert label2id['^'] == 1\n",
    "# for i in id2label:\n",
    "#     assert i == label2id[id2label[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "all_names = pd.read_csv('train_supervised_dataset.csv').fillna('')[['name', 'good', 'brand']].values.tolist()[:2000]\n",
    "all_names = [x[0] + ' <GD> '+ x[1] + ' <BR> ' + x[2] for x in all_names]\n",
    "    \n",
    "ln = len(all_names)\n",
    "\n",
    "shuffle(all_names)\n",
    "\n",
    "train = all_names[:int(ln*0.8)]\n",
    "val = all_names[int(ln*0.8):int(ln*0.9)]\n",
    "test = all_names[int(ln*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "all_names = pd.read_csv('train_unsupervised_dataset.csv')['name'].values.tolist()#[:100000]\n",
    "    \n",
    "ln = len(all_names)\n",
    "\n",
    "shuffle(all_names)\n",
    "\n",
    "train = all_names[:int(ln*0.8)]\n",
    "val = all_names[int(ln*0.8):int(ln*0.9)]\n",
    "test = all_names[int(ln*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3** Реализуйте функцию tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(name: str, add_sp_tokens = True) -> tp.List[int]:\n",
    "    \"\"\"принимает на вход имя, возвращает список из айдишников токенов. Так же добавляет токен начала последовательности в начало\n",
    "    и токен конца последовательности в конец\n",
    "    \"\"\"\n",
    "    \n",
    "    gd = name.rsplit('<GD>', 1)\n",
    "    if len(gd) == 2:\n",
    "        return [2] + tokenize(gd[0], False) + [4] + tokenize(gd[1], False) + [1]\n",
    "    \n",
    "    br = name.rsplit('<BR>', 1)\n",
    "    if len(br) == 2:\n",
    "        return tokenize(br[0], False) + [3] + tokenize(br[1], False)\n",
    "    \n",
    "    id_list = []\n",
    "    if add_sp_tokens:\n",
    "        id_list = [2]\n",
    "        \n",
    "    for letter in name:\n",
    "        id_list.append(label2id[letter])\n",
    "        \n",
    "    if add_sp_tokens:\n",
    "        id_list.append(1)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(list_: tp.List[int]):\n",
    "    return ''.join([id2label[x] for x in list_])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(names: tp.List[str]) -> torch.Tensor:\n",
    "    \"\"\"принимает на вход батч имён. Токенизирует их, и получившиеся списки токенов\n",
    "    объединяет в тензор, предварительно дополнив каждый из них нулями (индексом токена _) при необходимости\n",
    "\n",
    "    Для этого и пригодился токен _ -- чтобы можно было передавать последовательности разной длины в нейросеть\n",
    "    для параллельной обработки\n",
    "\n",
    "    возвращаемый тензор должен иметь размерность [максимальная длина, размер батча (len(names))]\n",
    "    \"\"\"\n",
    "    tokenized_batch = []\n",
    "    for name in names:\n",
    "        tokenized_name = tokenize(name)\n",
    "        tokenized_batch.append(tokenized_name)\n",
    "    max_len = max([len(x) for x in tokenized_batch])\n",
    "    \n",
    "    for idx, array in enumerate(tokenized_batch):\n",
    "        if len(array) != max_len:\n",
    "            tokenized_batch[idx] = array + [0]*(max_len-len(array))\n",
    "    \n",
    "    torch_tensor = torch.tensor(tokenized_batch).long()\n",
    "    \n",
    "    return torch.swapaxes(torch_tensor, 0, 1).to(DEVICE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNameGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_unique_tokens: int,\n",
    "        hidden_size: int, \n",
    "        embedding_size: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            number_of_unique_tokens,\n",
    "            embedding_size\n",
    "        )\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, number_of_unique_tokens)\n",
    "\n",
    "    def forward(self, prefix_token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Генерирует предсказания следующих токенов для prefix_token_ids\n",
    "\n",
    "        :param prefix_input_ids: тензор размера (максимальная длина в батче - 1, размер батча)\n",
    "        :type prefix_input_ids: torch.Tensor\n",
    "        :return: тензор размера (максимальная длина в батче - 1, размер батча, количество уникальных токенов)\n",
    "            важно, что должны возвращаться логиты, то есть то, что ДО softmax\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        hidden_states, last_state = self.rnn(self.embeddings(prefix_token_ids))\n",
    "        logits = self.classifier(hidden_states)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5\n",
    "Напишите лосс функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_function(\n",
    "    logits: torch.Tensor,\n",
    "    true_labels: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Считает кроссэнтропию, но усредненную только по позициям, где не символ '_'\n",
    "    (считаем, что учить предсказывать этот символ бессмысленно)\n",
    "    hint: используйте F.cross_entropy_loss c reduction='none', усредните лосс самостоятельно\n",
    "    :param logits: of shape[max_len - 1, batch_size, num_unique_tokens]\n",
    "    :type logits: torch.Tensor\n",
    "    :param true_labels: of shape[max_len - 1, batch_size]\n",
    "    :type true_labels: torch.Tensor\n",
    "    \"\"\"\n",
    "    #true_labels [max_seq_len, bs]\n",
    "    mask = true_labels == 0\n",
    "    \n",
    "    loss = F.cross_entropy(logits.permute(1, 2, 0), true_labels.T, reduction='none').T # [max_seq_len, bs]\n",
    "    \n",
    "    loss[mask] = 0\n",
    "    \n",
    "    mean = torch.mean(loss[~mask])\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявим вспомогательные классы для работы с даннными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, words: tp.List[str]):\n",
    "        self.words = words\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train)\n",
    "val_dataset = Dataset(val)\n",
    "test_dataset = Dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate,\n",
    "    drop_last=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 6\n",
    "Реализуйте функцию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prefix: str = ''):\n",
    "    current = tokenize(prefix)[:-1]  # начинаем с токена \"начало последовательности\"\n",
    "    while current[-1] != 1:\n",
    "        out = model(torch.unsqueeze(torch.tensor(current).long().to(DEVICE), 1)) #[max_in_seq_len, bs, num_of_char]   # получаем вероятности символов следующего токена\n",
    "        probs = torch.softmax(out[-1], dim=-1)  # сэмплируем этот токен пропорционально полученным вероятностям (torch.distributions.Categorical)\n",
    "        distr = torch.distributions.Categorical(probs=probs)\n",
    "        next = distr.sample()\n",
    "        current.append(next.item())\n",
    "    \n",
    "    return \"\".join([id2label[c] for c in current[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "hidden_size = 64\n",
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNNameGenerator(\n",
    "    number_of_unique_tokens=len(id2label),\n",
    "    hidden_size=hidden_size,\n",
    "    embedding_size=embedding_size,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Коровка из Кореновки мороженое 90г ваниль <GD>4Н$k\\x07'ъЇG8г\\x01Pgi!р\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, 'Коровка из Кореновки мороженое 90г ваниль <GD>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 7 \n",
    "Всё вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m     train_step\u001b[38;5;241m.\u001b[39mappend(global_step)\n\u001b[0;32m---> 24\u001b[0m     train_loss_history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     26\u001b[0m total_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_batch \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_step = []\n",
    "validation_step = []\n",
    "train_loss_history = []\n",
    "validation_loss_history = []\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(epoch)\n",
    "    for train_batch in train_dataloader:\n",
    "        # если хотим для каждого токена кроме последнего предсказать следующий, то что\n",
    "        # здесь на вход модели, а что на выход? (не стесняйтесь просить хинты)\n",
    "        x, y = train_batch[:-1,:], train_batch[1:,:]\n",
    "        # считаем логиты\n",
    "        logits = model(x)\n",
    "        # считаем лосс\n",
    "        loss = calculate_loss_function(logits, y)\n",
    "        # backward шаг\n",
    "        loss.backward()\n",
    "        # шаг оптимизатора и зануление старых градиентов\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        train_step.append(global_step)\n",
    "        train_loss_history.append(loss.item()) \n",
    "    \n",
    "    total_val_loss = 0.0\n",
    "    for val_batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            x, y = val_batch[:-1,:], val_batch[1:,:]\n",
    "            logits = model(x)\n",
    "            loss = calculate_loss_function(logits, y)\n",
    "            total_val_loss += loss.item()\n",
    "    total_val_loss /= len(val_dataloader)\n",
    "    validation_step.append(global_step)\n",
    "    validation_loss_history.append(total_val_loss)\n",
    "\n",
    "    clear_output()\n",
    "    plt.plot(train_step, train_loss_history, label='train')\n",
    "    plt.plot(validation_step, validation_loss_history, label='validation')\n",
    "    plt.xlabel('training step')\n",
    "    plt.ylabel('cross entropy')\n",
    "    plt.title('training name generator')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_loss = 0.0\n",
    "for test_batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x, y = test_batch[:-1,:], test_batch[1:,:]\n",
    "        logits = model(x)\n",
    "        loss = calculate_loss_function(logits, y)\n",
    "        total_test_loss += loss.item()\n",
    "total_test_loss /= len(test_dataloader)\n",
    "\n",
    "\n",
    "assert total_test_loss < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated names\n",
      "Слингкои <BR> diw LM2271990-63005, БРЬНиковый Ciriby, (02.27,53 (1318658GNiR) <GD> шоколадная гуранио д/мель <BR> atalexaag Tri Syrert Mepel Tewe LMBERANY/932 <GD> март. гурт. курпины ФИФЛАЛЬ МАМ D FFans Бир <BR> \n",
      "G KWRA <GD> мармелый мостаж (OSCONE 62-70 <GD> сигарной (73) <GD> <GD> бизкина <BR> ноское 207мм 56г <GD> алгушка <BR> \n",
      "ТЫЕ коколм 310гр <GD> шоколодени Обушя бивона 700., S 1300K90 Пак с с мми <BR> хотеменьные <GD> церлеблены \"Пистрай,йофит-чирс <GD> вощеной 5756195966932-VSI 001246--619 Storck/BIILER  858л мис <BR> \n",
      "[2] УБ  гужценый №1 2*9455/26*30 шом  e*16076k2 <GD>  <GD> крешок HLВ 180-000 <GD> лобо <BR> lacw\n",
      "Пирожка <BR> рукил <BR> деслое серок <BR> semty\n",
      "756274 G\" 160/40 <GD> вигнямия Ifschx\n",
      "Прамем 3110-87116 К300088676 <GD> сером\" \"Breumantin 40 <GD> косные кюрг\n",
      "0466 - Рокт Болко\n",
      "Ябава Яглизломи <BR> palei\n",
      "ЛЮЧНУ ЇTS15.6 БОARIRI Маслоко джьео) <GD> бумых 1 сметинь <BR> дримора аздинье Аз <BR> \n",
      "Кикевнячики КПР 1600 <GD> плост.Оретш \"KLblyesh gal) перем Пакажа в архаме/ 0,1% <GD> батчин тавархимо\n",
      "Шок смет.   БПючка <BR> \n",
      "All' <BR> \n",
      "96М11835827.000810725-19031-39 <GD> финтюво 45мг <GD> бид 300г МА) <GD> реклотья морода (Meutikp\n",
      "Мернничия ми Оскорс <GD> папови\n",
      "Пиво носэщейские нач <BR> \n",
      "15007700М2 Сл 25гр мятолизат <GD> чидом\" хeрто) 12+,31 <GD> соба <BR> мис+ жез <BR> \n",
      "ЦИОТОц/ (1-7 59881-698 конфенит туфрофнин\n",
      "Снеми акфа <BR> haucti V110132,0D136 M) <GD> баталый 32х14-4 <GD> паклот цучка <BR> меифка апуфель Челка <BR> marol tosss\n",
      "СУАЛИМИКА Saveldich Фолбалья 140мл 1шт изота д/плай мурта 90) <GD> геллебо\" мостеммана <BR> \n"
     ]
    }
   ],
   "source": [
    "print(\"Generated names\")  # обученная генерация\n",
    "for _ in range(20):\n",
    "    print(generate(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(pred: str) -> tp.Tuple[str, str]:\n",
    "    good_and_brand = pred.split('<GD>')[1]\n",
    "    good_brand = good_and_brand.split('<BR>', 1)\n",
    "    if len(good_brand) == 2:\n",
    "        good, brand = good_brand\n",
    "    else:\n",
    "        good = good_brand[0]\n",
    "        brand = ''\n",
    "    \n",
    "    brand = brand.replace('<BR>', ',')\n",
    "    brand = brand.replace(' ,', ',')\n",
    "    \n",
    "    return good.strip(), brand.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('крул COЦ - 6 спрафит пасаревой дайные Zurion chelumin trama', '')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities(generate(model, 'Коровка из Кореновки <GD>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = test_frame['name']\n",
    "names = names.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame.drop(columns=['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, elem in enumerate(names):\n",
    "    good, brand = extract_entities(generate(model, elem + ' <GD>'))\n",
    "    test_frame.loc[idx, 'good'] = good\n",
    "    test_frame.loc[idx, 'brand'] = brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b57e1ee3ce2d9880d8a1431cc6218bd6dd8233a03980e7e673f351183fa295a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
