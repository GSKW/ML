{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разминки:\n",
    "###  Задание 1\n",
    "\n",
    "Посчитай направление наибольшего роста функции $f(x, y) = 3x^2 + y^2$ в точке $z_0 = (1, 3train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f'(x,y) = 6x+2y\n",
    "f'_x(x,y) = 6x\n",
    "f'_y(x,y) = 2y\n",
    "\n",
    "f'_x(x,y) = 6x^2 # 6\n",
    "f'_y(x,y) = 3y^2 # 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.) tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "def func(x,y):\n",
    "    return 3*x**2+y**2\n",
    "\n",
    "def func2(x,y):\n",
    "    return 2*x**3+y**3\n",
    "\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "result = func2(x, y)\n",
    "result.backward()\n",
    "print(x.grad, y.grad)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Задание 2\n",
    "\n",
    "Найди минимум функции $f(x, y) = 3x^2 + y^2$."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f'(x,y) = 6x+2y = 0 # y = -3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.) tensor(6.)\n",
      "tensor(0.4000, requires_grad=True) tensor(2.4000, requires_grad=True) tensor(6.2400, grad_fn=<AddBackward0>)\n",
      "tensor(2.4000) tensor(4.8000)\n",
      "tensor(0.1600, requires_grad=True) tensor(1.9200, requires_grad=True) tensor(3.7632, grad_fn=<AddBackward0>)\n",
      "tensor(0.9600) tensor(3.8400)\n",
      "tensor(0.0640, requires_grad=True) tensor(1.5360, requires_grad=True) tensor(2.3716, grad_fn=<AddBackward0>)\n",
      "tensor(0.3840) tensor(3.0720)\n",
      "tensor(0.0256, requires_grad=True) tensor(1.2288, requires_grad=True) tensor(1.5119, grad_fn=<AddBackward0>)\n",
      "tensor(0.1536) tensor(2.4576)\n",
      "tensor(0.0102, requires_grad=True) tensor(0.9830, requires_grad=True) tensor(0.9667, grad_fn=<AddBackward0>)\n",
      "tensor(0.0614) tensor(1.9661)\n",
      "tensor(0.0041, requires_grad=True) tensor(0.7864, requires_grad=True) tensor(0.6185, grad_fn=<AddBackward0>)\n",
      "tensor(0.0246) tensor(1.5729)\n",
      "tensor(0.0016, requires_grad=True) tensor(0.6291, requires_grad=True) tensor(0.3958, grad_fn=<AddBackward0>)\n",
      "tensor(0.0098) tensor(1.2583)\n",
      "tensor(0.0007, requires_grad=True) tensor(0.5033, requires_grad=True) tensor(0.2533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039) tensor(1.0066)\n",
      "tensor(0.0003, requires_grad=True) tensor(0.4027, requires_grad=True) tensor(0.1621, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016) tensor(0.8053)\n",
      "tensor(0.0001, requires_grad=True) tensor(0.3221, requires_grad=True) tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006) tensor(0.6442)\n",
      "tensor(4.1943e-05, requires_grad=True) tensor(0.2577, requires_grad=True) tensor(0.0664, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003) tensor(0.5154)\n",
      "tensor(1.6777e-05, requires_grad=True) tensor(0.2062, requires_grad=True) tensor(0.0425, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001) tensor(0.4123)\n",
      "tensor(6.7109e-06, requires_grad=True) tensor(0.1649, requires_grad=True) tensor(0.0272, grad_fn=<AddBackward0>)\n",
      "tensor(4.0265e-05) tensor(0.3299)\n",
      "tensor(2.6844e-06, requires_grad=True) tensor(0.1319, requires_grad=True) tensor(0.0174, grad_fn=<AddBackward0>)\n",
      "tensor(1.6106e-05) tensor(0.2639)\n",
      "tensor(1.0737e-06, requires_grad=True) tensor(0.1056, requires_grad=True) tensor(0.0111, grad_fn=<AddBackward0>)\n",
      "tensor(6.4424e-06) tensor(0.2111)\n",
      "tensor(4.2950e-07, requires_grad=True) tensor(0.0844, requires_grad=True) tensor(0.0071, grad_fn=<AddBackward0>)\n",
      "tensor(2.5770e-06) tensor(0.1689)\n",
      "tensor(1.7180e-07, requires_grad=True) tensor(0.0676, requires_grad=True) tensor(0.0046, grad_fn=<AddBackward0>)\n",
      "tensor(1.0308e-06) tensor(0.1351)\n",
      "tensor(6.8719e-08, requires_grad=True) tensor(0.0540, requires_grad=True) tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "tensor(4.1232e-07) tensor(0.1081)\n",
      "tensor(2.7488e-08, requires_grad=True) tensor(0.0432, requires_grad=True) tensor(0.0019, grad_fn=<AddBackward0>)\n",
      "tensor(1.6493e-07) tensor(0.0865)\n",
      "tensor(1.0995e-08, requires_grad=True) tensor(0.0346, requires_grad=True) tensor(0.0012, grad_fn=<AddBackward0>)\n",
      "tensor(6.5971e-08) tensor(0.0692)\n",
      "tensor(4.3980e-09, requires_grad=True) tensor(0.0277, requires_grad=True) tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "tensor(2.6388e-08) tensor(0.0553)\n",
      "tensor(1.7592e-09, requires_grad=True) tensor(0.0221, requires_grad=True) tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "tensor(1.0555e-08) tensor(0.0443)\n",
      "tensor(7.0369e-10, requires_grad=True) tensor(0.0177, requires_grad=True) tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "tensor(4.2221e-09) tensor(0.0354)\n",
      "tensor(2.8147e-10, requires_grad=True) tensor(0.0142, requires_grad=True) tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "tensor(1.6888e-09) tensor(0.0283)\n",
      "tensor(1.1259e-10, requires_grad=True) tensor(0.0113, requires_grad=True) tensor(0.0001, grad_fn=<AddBackward0>)\n",
      "tensor(6.7554e-10) tensor(0.0227)\n",
      "tensor(4.5036e-11, requires_grad=True) tensor(0.0091, requires_grad=True) tensor(8.2209e-05, grad_fn=<AddBackward0>)\n",
      "tensor(2.7022e-10) tensor(0.0181)\n",
      "tensor(1.8014e-11, requires_grad=True) tensor(0.0073, requires_grad=True) tensor(5.2614e-05, grad_fn=<AddBackward0>)\n",
      "tensor(1.0809e-10) tensor(0.0145)\n",
      "tensor(7.2057e-12, requires_grad=True) tensor(0.0058, requires_grad=True) tensor(3.3673e-05, grad_fn=<AddBackward0>)\n",
      "tensor(4.3234e-11) tensor(0.0116)\n",
      "tensor(2.8823e-12, requires_grad=True) tensor(0.0046, requires_grad=True) tensor(2.1551e-05, grad_fn=<AddBackward0>)\n",
      "tensor(1.7294e-11) tensor(0.0093)\n",
      "tensor(1.1529e-12, requires_grad=True) tensor(0.0037, requires_grad=True) tensor(1.3792e-05, grad_fn=<AddBackward0>)\n",
      "tensor(6.9175e-12) tensor(0.0074)\n",
      "tensor(4.6117e-13, requires_grad=True) tensor(0.0030, requires_grad=True) tensor(8.8272e-06, grad_fn=<AddBackward0>)\n",
      "tensor(2.7670e-12) tensor(0.0059)\n",
      "tensor(1.8447e-13, requires_grad=True) tensor(0.0024, requires_grad=True) tensor(5.6494e-06, grad_fn=<AddBackward0>)\n",
      "tensor(1.1068e-12) tensor(0.0048)\n",
      "tensor(7.3787e-14, requires_grad=True) tensor(0.0019, requires_grad=True) tensor(3.6156e-06, grad_fn=<AddBackward0>)\n",
      "tensor(4.4272e-13) tensor(0.0038)\n",
      "tensor(2.9515e-14, requires_grad=True) tensor(0.0015, requires_grad=True) tensor(2.3140e-06, grad_fn=<AddBackward0>)\n",
      "tensor(1.7709e-13) tensor(0.0030)\n",
      "tensor(1.1806e-14, requires_grad=True) tensor(0.0012, requires_grad=True) tensor(1.4810e-06, grad_fn=<AddBackward0>)\n",
      "tensor(7.0835e-14) tensor(0.0024)\n",
      "tensor(4.7224e-15, requires_grad=True) tensor(0.0010, requires_grad=True) tensor(9.4781e-07, grad_fn=<AddBackward0>)\n",
      "tensor(2.8334e-14) tensor(0.0019)\n",
      "tensor(1.8889e-15, requires_grad=True) tensor(0.0008, requires_grad=True) tensor(6.0660e-07, grad_fn=<AddBackward0>)\n",
      "tensor(1.1334e-14) tensor(0.0016)\n",
      "tensor(7.5558e-16, requires_grad=True) tensor(0.0006, requires_grad=True) tensor(3.8822e-07, grad_fn=<AddBackward0>)\n",
      "tensor(4.5335e-15) tensor(0.0012)\n",
      "tensor(3.0223e-16, requires_grad=True) tensor(0.0005, requires_grad=True) tensor(2.4846e-07, grad_fn=<AddBackward0>)\n",
      "tensor(1.8134e-15) tensor(0.0010)\n",
      "tensor(1.2089e-16, requires_grad=True) tensor(0.0004, requires_grad=True) tensor(1.5902e-07, grad_fn=<AddBackward0>)\n",
      "tensor(7.2535e-16) tensor(0.0008)\n",
      "tensor(4.8357e-17, requires_grad=True) tensor(0.0003, requires_grad=True) tensor(1.0177e-07, grad_fn=<AddBackward0>)\n",
      "tensor(2.9014e-16) tensor(0.0006)\n",
      "tensor(1.9343e-17, requires_grad=True) tensor(0.0003, requires_grad=True) tensor(6.5133e-08, grad_fn=<AddBackward0>)\n",
      "tensor(1.1606e-16) tensor(0.0005)\n",
      "tensor(7.7371e-18, requires_grad=True) tensor(0.0002, requires_grad=True) tensor(4.1685e-08, grad_fn=<AddBackward0>)\n",
      "tensor(4.6423e-17) tensor(0.0004)\n",
      "tensor(3.0948e-18, requires_grad=True) tensor(0.0002, requires_grad=True) tensor(2.6678e-08, grad_fn=<AddBackward0>)\n",
      "tensor(1.8569e-17) tensor(0.0003)\n",
      "tensor(1.2379e-18, requires_grad=True) tensor(0.0001, requires_grad=True) tensor(1.7074e-08, grad_fn=<AddBackward0>)\n",
      "tensor(7.4276e-18) tensor(0.0003)\n",
      "tensor(4.9518e-19, requires_grad=True) tensor(0.0001, requires_grad=True) tensor(1.0928e-08, grad_fn=<AddBackward0>)\n",
      "tensor(2.9711e-18) tensor(0.0002)\n",
      "tensor(1.9807e-19, requires_grad=True) tensor(8.3628e-05, requires_grad=True) tensor(6.9936e-09, grad_fn=<AddBackward0>)\n",
      "tensor(1.1884e-18) tensor(0.0002)\n",
      "tensor(7.9228e-20, requires_grad=True) tensor(6.6902e-05, requires_grad=True) tensor(4.4759e-09, grad_fn=<AddBackward0>)\n",
      "tensor(4.7537e-19) tensor(0.0001)\n",
      "tensor(3.1691e-20, requires_grad=True) tensor(5.3522e-05, requires_grad=True) tensor(2.8646e-09, grad_fn=<AddBackward0>)\n",
      "tensor(1.9015e-19) tensor(0.0001)\n",
      "tensor(1.2676e-20, requires_grad=True) tensor(4.2817e-05, requires_grad=True) tensor(1.8333e-09, grad_fn=<AddBackward0>)\n",
      "tensor(7.6059e-20) tensor(8.5635e-05)\n",
      "tensor(5.0706e-21, requires_grad=True) tensor(3.4254e-05, requires_grad=True) tensor(1.1733e-09, grad_fn=<AddBackward0>)\n",
      "tensor(3.0424e-20) tensor(6.8508e-05)\n",
      "tensor(2.0282e-21, requires_grad=True) tensor(2.7403e-05, requires_grad=True) tensor(7.5093e-10, grad_fn=<AddBackward0>)\n",
      "tensor(1.2169e-20) tensor(5.4806e-05)\n",
      "tensor(8.1130e-22, requires_grad=True) tensor(2.1923e-05, requires_grad=True) tensor(4.8060e-10, grad_fn=<AddBackward0>)\n",
      "tensor(4.8678e-21) tensor(4.3845e-05)\n",
      "tensor(3.2452e-22, requires_grad=True) tensor(1.7538e-05, requires_grad=True) tensor(3.0758e-10, grad_fn=<AddBackward0>)\n",
      "tensor(1.9471e-21) tensor(3.5076e-05)\n",
      "tensor(1.2981e-22, requires_grad=True) tensor(1.4030e-05, requires_grad=True) tensor(1.9685e-10, grad_fn=<AddBackward0>)\n",
      "tensor(7.7884e-22) tensor(2.8061e-05)\n",
      "tensor(5.1923e-23, requires_grad=True) tensor(1.1224e-05, requires_grad=True) tensor(1.2599e-10, grad_fn=<AddBackward0>)\n",
      "tensor(3.1154e-22) tensor(2.2449e-05)\n",
      "tensor(2.0769e-23, requires_grad=True) tensor(8.9795e-06, requires_grad=True) tensor(8.0631e-11, grad_fn=<AddBackward0>)\n",
      "tensor(1.2461e-22) tensor(1.7959e-05)\n",
      "tensor(8.3077e-24, requires_grad=True) tensor(7.1836e-06, requires_grad=True) tensor(5.1604e-11, grad_fn=<AddBackward0>)\n",
      "tensor(4.9846e-23) tensor(1.4367e-05)\n",
      "tensor(3.3231e-24, requires_grad=True) tensor(5.7469e-06, requires_grad=True) tensor(3.3026e-11, grad_fn=<AddBackward0>)\n",
      "tensor(1.9938e-23) tensor(1.1494e-05)\n",
      "tensor(1.3292e-24, requires_grad=True) tensor(4.5975e-06, requires_grad=True) tensor(2.1137e-11, grad_fn=<AddBackward0>)\n",
      "tensor(7.9754e-24) tensor(9.1950e-06)\n",
      "tensor(5.3169e-25, requires_grad=True) tensor(3.6780e-06, requires_grad=True) tensor(1.3528e-11, grad_fn=<AddBackward0>)\n",
      "tensor(3.1901e-24) tensor(7.3560e-06)\n",
      "tensor(2.1268e-25, requires_grad=True) tensor(2.9424e-06, requires_grad=True) tensor(8.6577e-12, grad_fn=<AddBackward0>)\n",
      "tensor(1.2761e-24) tensor(5.8848e-06)\n",
      "tensor(8.5070e-26, requires_grad=True) tensor(2.3539e-06, requires_grad=True) tensor(5.5409e-12, grad_fn=<AddBackward0>)\n",
      "tensor(5.1042e-25) tensor(4.7078e-06)\n",
      "tensor(3.4028e-26, requires_grad=True) tensor(1.8831e-06, requires_grad=True) tensor(3.5462e-12, grad_fn=<AddBackward0>)\n",
      "tensor(2.0417e-25) tensor(3.7663e-06)\n",
      "tensor(1.3611e-26, requires_grad=True) tensor(1.5065e-06, requires_grad=True) tensor(2.2696e-12, grad_fn=<AddBackward0>)\n",
      "tensor(8.1668e-26) tensor(3.0130e-06)\n",
      "tensor(5.4445e-27, requires_grad=True) tensor(1.2052e-06, requires_grad=True) tensor(1.4525e-12, grad_fn=<AddBackward0>)\n",
      "tensor(3.2667e-26) tensor(2.4104e-06)\n",
      "tensor(2.1778e-27, requires_grad=True) tensor(9.6416e-07, requires_grad=True) tensor(9.2961e-13, grad_fn=<AddBackward0>)\n",
      "tensor(1.3067e-26) tensor(1.9283e-06)\n",
      "tensor(8.7112e-28, requires_grad=True) tensor(7.7133e-07, requires_grad=True) tensor(5.9495e-13, grad_fn=<AddBackward0>)\n",
      "tensor(5.2267e-27) tensor(1.5427e-06)\n",
      "tensor(3.4845e-28, requires_grad=True) tensor(6.1706e-07, requires_grad=True) tensor(3.8077e-13, grad_fn=<AddBackward0>)\n",
      "tensor(2.0907e-27) tensor(1.2341e-06)\n",
      "tensor(1.3938e-28, requires_grad=True) tensor(4.9365e-07, requires_grad=True) tensor(2.4369e-13, grad_fn=<AddBackward0>)\n",
      "tensor(8.3628e-28) tensor(9.8730e-07)\n",
      "tensor(5.5752e-29, requires_grad=True) tensor(3.9492e-07, requires_grad=True) tensor(1.5596e-13, grad_fn=<AddBackward0>)\n",
      "tensor(3.3451e-28) tensor(7.8984e-07)\n",
      "tensor(2.2301e-29, requires_grad=True) tensor(3.1594e-07, requires_grad=True) tensor(9.9816e-14, grad_fn=<AddBackward0>)\n",
      "tensor(1.3380e-28) tensor(6.3187e-07)\n",
      "tensor(8.9203e-30, requires_grad=True) tensor(2.5275e-07, requires_grad=True) tensor(6.3882e-14, grad_fn=<AddBackward0>)\n",
      "tensor(5.3522e-29) tensor(5.0550e-07)\n",
      "tensor(3.5681e-30, requires_grad=True) tensor(2.0220e-07, requires_grad=True) tensor(4.0885e-14, grad_fn=<AddBackward0>)\n",
      "tensor(2.1409e-29) tensor(4.0440e-07)\n",
      "tensor(1.4272e-30, requires_grad=True) tensor(1.6176e-07, requires_grad=True) tensor(2.6166e-14, grad_fn=<AddBackward0>)\n",
      "tensor(8.5635e-30) tensor(3.2352e-07)\n",
      "tensor(5.7090e-31, requires_grad=True) tensor(1.2941e-07, requires_grad=True) tensor(1.6746e-14, grad_fn=<AddBackward0>)\n",
      "tensor(3.4254e-30) tensor(2.5882e-07)\n",
      "tensor(2.2836e-31, requires_grad=True) tensor(1.0353e-07, requires_grad=True) tensor(1.0718e-14, grad_fn=<AddBackward0>)\n",
      "tensor(1.3702e-30) tensor(2.0705e-07)\n",
      "tensor(9.1344e-32, requires_grad=True) tensor(8.2821e-08, requires_grad=True) tensor(6.8593e-15, grad_fn=<AddBackward0>)\n",
      "tensor(5.4806e-31) tensor(1.6564e-07)\n",
      "tensor(3.6537e-32, requires_grad=True) tensor(6.6257e-08, requires_grad=True) tensor(4.3900e-15, grad_fn=<AddBackward0>)\n",
      "tensor(2.1922e-31) tensor(1.3251e-07)\n",
      "tensor(1.4615e-32, requires_grad=True) tensor(5.3005e-08, requires_grad=True) tensor(2.8096e-15, grad_fn=<AddBackward0>)\n",
      "tensor(8.7690e-32) tensor(1.0601e-07)\n",
      "tensor(5.8460e-33, requires_grad=True) tensor(4.2404e-08, requires_grad=True) tensor(1.7981e-15, grad_fn=<AddBackward0>)\n",
      "tensor(3.5076e-32) tensor(8.4809e-08)\n",
      "tensor(2.3384e-33, requires_grad=True) tensor(3.3923e-08, requires_grad=True) tensor(1.1508e-15, grad_fn=<AddBackward0>)\n",
      "tensor(1.4030e-32) tensor(6.7847e-08)\n",
      "tensor(9.3536e-34, requires_grad=True) tensor(2.7139e-08, requires_grad=True) tensor(7.3651e-16, grad_fn=<AddBackward0>)\n",
      "tensor(5.6122e-33) tensor(5.4278e-08)\n",
      "tensor(3.7414e-34, requires_grad=True) tensor(2.1711e-08, requires_grad=True) tensor(4.7137e-16, grad_fn=<AddBackward0>)\n",
      "tensor(2.2449e-33) tensor(4.3422e-08)\n",
      "tensor(1.4966e-34, requires_grad=True) tensor(1.7369e-08, requires_grad=True) tensor(3.0168e-16, grad_fn=<AddBackward0>)\n",
      "tensor(8.9794e-34) tensor(3.4738e-08)\n",
      "tensor(5.9863e-35, requires_grad=True) tensor(1.3895e-08, requires_grad=True) tensor(1.9307e-16, grad_fn=<AddBackward0>)\n",
      "tensor(3.5918e-34) tensor(2.7790e-08)\n",
      "tensor(2.3945e-35, requires_grad=True) tensor(1.1116e-08, requires_grad=True) tensor(1.2357e-16, grad_fn=<AddBackward0>)\n",
      "tensor(1.4367e-34) tensor(2.2232e-08)\n",
      "tensor(9.5781e-36, requires_grad=True) tensor(8.8928e-09, requires_grad=True) tensor(7.9082e-17, grad_fn=<AddBackward0>)\n",
      "tensor(5.7468e-35) tensor(1.7786e-08)\n",
      "tensor(3.8312e-36, requires_grad=True) tensor(7.1143e-09, requires_grad=True) tensor(5.0613e-17, grad_fn=<AddBackward0>)\n",
      "tensor(2.2987e-35) tensor(1.4229e-08)\n",
      "tensor(1.5325e-36, requires_grad=True) tensor(5.6914e-09, requires_grad=True) tensor(3.2392e-17, grad_fn=<AddBackward0>)\n",
      "tensor(9.1949e-36) tensor(1.1383e-08)\n",
      "tensor(6.1300e-37, requires_grad=True) tensor(4.5531e-09, requires_grad=True) tensor(2.0731e-17, grad_fn=<AddBackward0>)\n",
      "tensor(3.6780e-36) tensor(9.1063e-09)\n",
      "tensor(2.4520e-37, requires_grad=True) tensor(3.6425e-09, requires_grad=True) tensor(1.3268e-17, grad_fn=<AddBackward0>)\n",
      "tensor(1.4712e-36) tensor(7.2850e-09)\n",
      "tensor(9.8079e-38, requires_grad=True) tensor(2.9140e-09, requires_grad=True) tensor(8.4914e-18, grad_fn=<AddBackward0>)\n",
      "tensor(5.8848e-37) tensor(5.8280e-09)\n",
      "tensor(3.9232e-38, requires_grad=True) tensor(2.3312e-09, requires_grad=True) tensor(5.4345e-18, grad_fn=<AddBackward0>)\n",
      "tensor(2.3539e-37) tensor(4.6624e-09)\n",
      "tensor(1.5693e-38, requires_grad=True) tensor(1.8650e-09, requires_grad=True) tensor(3.4781e-18, grad_fn=<AddBackward0>)\n",
      "tensor(9.4156e-38) tensor(3.7299e-09)\n",
      "tensor(6.2771e-39, requires_grad=True) tensor(1.4920e-09, requires_grad=True) tensor(2.2260e-18, grad_fn=<AddBackward0>)\n",
      "tensor(3.7663e-38) tensor(2.9839e-09)\n",
      "tensor(2.5108e-39, requires_grad=True) tensor(1.1936e-09, requires_grad=True) tensor(1.4246e-18, grad_fn=<AddBackward0>)\n",
      "tensor(1.5065e-38) tensor(2.3872e-09)\n",
      "tensor(1.0043e-39, requires_grad=True) tensor(9.5486e-10, requires_grad=True) tensor(9.1176e-19, grad_fn=<AddBackward0>)\n",
      "tensor(6.0260e-39) tensor(1.9097e-09)\n",
      "tensor(4.0173e-40, requires_grad=True) tensor(7.6389e-10, requires_grad=True) tensor(5.8353e-19, grad_fn=<AddBackward0>)\n",
      "tensor(2.4104e-39) tensor(1.5278e-09)\n",
      "tensor(1.6069e-40, requires_grad=True) tensor(6.1111e-10, requires_grad=True) tensor(3.7346e-19, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):    \n",
    "    result = func(x, y)\n",
    "    result.backward()\n",
    "    print(x.grad, y.grad)\n",
    "    x.data -= x.grad*0.1\n",
    "    y.data -= y.grad*0.1\n",
    "    print(x, y, func(x,y))\n",
    "    x.grad.data.zero_()\n",
    "    y.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Высокоуровневый PyTorch\n",
    "\n",
    "В этом ноутбуке мы не будем вручную создавать переменные и вручную считать производные. В торче есть много абстракций и инструментов, которые облегчают жизнь (хоть и вручную можно создать что угодно)\n",
    "\n",
    "Самая главная из них это nn.Module.Посмотрим на его документацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base class for all neural network modules.\n",
      "\n",
      "    Your models should also subclass this class.\n",
      "\n",
      "    Modules can also contain other Modules, allowing to nest them in\n",
      "    a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "        import torch.nn as nn\n",
      "        import torch.nn.functional as F\n",
      "\n",
      "        class Model(nn.Module):\n",
      "            def __init__(self):\n",
      "                super().__init__()\n",
      "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "            def forward(self, x):\n",
      "                x = F.relu(self.conv1(x))\n",
      "                return F.relu(self.conv2(x))\n",
      "\n",
      "    Submodules assigned in this way will be registered, and will have their\n",
      "    parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      "    .. note::\n",
      "        As per the example above, an ``__init__()`` call to the parent class\n",
      "        must be made before assignment on the child.\n",
      "\n",
      "    :ivar training: Boolean represents whether this module is in training or\n",
      "                    evaluation mode.\n",
      "    :vartype training: bool\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(nn.Module.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства сделано, что все архитектуры (\"модули\") написанные вами на pytorch-e должны быть отнаследованы от nn.Module.\n",
    "\n",
    "Каждый модуль -- это нечто, что принимает на вход какие-то тензоры, и возвращает какие-то тензоры. Это как раз forward pass -- его и нужно описывать в методе `forward`. В примере выше описана нейросеть, в которой два слоя (self.conv1 и self.conv2). В forward() описана логика того, как должен быть предобработан входной тензор `x`. \n",
    "В примере выше сначала применяется первый слой, потом функция активации, потом второй слой, а потом еще раз функция активации.\n",
    "\n",
    "Многие классы из torch.nn являются наследниками `nn.Module`-я -- их и рекомендуется использовать в качестве \"строительных блоков\" вашей нейросети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # не стоит забывать так писать\n",
    "        self.linear_first = nn.Linear(64, 64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear_second = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.linear_first(inp)\n",
    "        result = self.activation(result)\n",
    "        result = self.linear_second(result)\n",
    "        result = self.activation(result) \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = Block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(100, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Module-и можно вызывать прямо так, и под капотом будет вызвано то, что реализовано в методе .forward()\n",
    "block(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block(inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другой способ создать ту же самую нейросеть с помощью строительного блока `nn.Sequential`. Он сделан для того, чтобы применять подряд другие блоки, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_seq = nn.Sequential(\n",
    "    nn.Linear(64, 64),  # линейный слой 64 признака в 64 признака\n",
    "    nn.ReLU(),  # функция активации\n",
    "    nn.Linear(64, 1),  # линейный слой 64 признака в 1 признак\n",
    "    nn.ReLU(),  #  еще одна функция активации\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118],\n",
       "        [0.1118]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_seq(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_seq(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(block(inp), block_seq(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Почему в последней ячейке вывелось False?**. Веса нейронной сети для хорошей оптимизации обычно инициализируют рандомно, причем инициализация должна быть *правильной*.\n",
    "В торче стратегии инициализации весов уже реализованы за вас, и если объявлять модули, то веса модуля уже будут рандомно инициализированы. Так как они рандомно инициализированы, то и выход двух заново созданных модулей (`block` и `block_seq`) будет разным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В pytorch так же есть реализации наиболее известных оптимизаторов -- например, стохастический градиентный спуск.\n",
    "Чтобы инициализировать почти любой оптимизатор, ему нужно передать список из тензоров (или словарей тензоров), которые нужно оптимизировать.\n",
    "Чтобы получить веса nn.Module нужно вызвать у него метод parameters()\n",
    "Так, чтобы оптимизировать веса `block`-а, подойдет например вот такой оптимизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(block.parameters(), lr=0.05)\n",
    "optimizer1 = Adam(block.parameters(), lr=0.05)\n",
    "#  сюда мы так же передали lr -- наш learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linear_first.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0976,  0.1048,  0.0307,  ...,  0.0450, -0.1108, -0.1220],\n",
       "          [ 0.1033, -0.0756,  0.0891,  ..., -0.0987,  0.0437,  0.1142],\n",
       "          [ 0.1087, -0.1066, -0.0709,  ..., -0.1084, -0.0075,  0.0911],\n",
       "          ...,\n",
       "          [ 0.0504,  0.0190, -0.0433,  ..., -0.1018, -0.1243,  0.0382],\n",
       "          [-0.0776,  0.0034,  0.0421,  ...,  0.0346, -0.0780, -0.0010],\n",
       "          [-0.0161, -0.0373,  0.0353,  ...,  0.1208, -0.0905,  0.1169]],\n",
       "         requires_grad=True)),\n",
       " ('linear_first.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0462, -0.0991,  0.0340, -0.0923,  0.0256, -0.0233, -0.0558, -0.0358,\n",
       "           0.0645,  0.0598,  0.1152,  0.0360,  0.0945, -0.0561, -0.0071,  0.0733,\n",
       "          -0.1237, -0.1072,  0.0006, -0.0820,  0.1008,  0.0500,  0.0377, -0.1071,\n",
       "          -0.0875, -0.0605,  0.0227,  0.0350, -0.0115, -0.0486, -0.0513, -0.0974,\n",
       "           0.0626, -0.0455,  0.0537,  0.0352, -0.0813,  0.0348,  0.0484,  0.0910,\n",
       "           0.0775,  0.0715,  0.0258,  0.0716, -0.0792,  0.0690,  0.0973, -0.1208,\n",
       "          -0.0500, -0.0770, -0.0826, -0.1084,  0.1007,  0.0897,  0.0832,  0.0488,\n",
       "          -0.0733, -0.1091, -0.1225, -0.0882,  0.0416, -0.0572, -0.0467, -0.0360],\n",
       "         requires_grad=True)),\n",
       " ('linear_second.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.1217, -0.1086, -0.0609,  0.0717, -0.0539, -0.1208, -0.1062, -0.0180,\n",
       "           -0.0097, -0.0030, -0.0598,  0.0482, -0.0974, -0.1147, -0.1244,  0.1102,\n",
       "            0.0744,  0.0624,  0.0635,  0.0074, -0.0427,  0.0247, -0.1237,  0.0731,\n",
       "           -0.0543,  0.0163, -0.0265,  0.0904, -0.1173, -0.0556, -0.1187,  0.0353,\n",
       "            0.0893, -0.0698,  0.0101, -0.0564, -0.0320, -0.0400, -0.0433, -0.0866,\n",
       "           -0.0498, -0.0172, -0.0455, -0.0252,  0.1135,  0.1220, -0.0159, -0.1064,\n",
       "            0.0296, -0.0121, -0.0640,  0.0377,  0.0681, -0.0861, -0.0674, -0.0949,\n",
       "           -0.0092,  0.0166, -0.0672,  0.0138,  0.1014,  0.0972, -0.0693, -0.0544]],\n",
       "         requires_grad=True)),\n",
       " ('linear_second.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1028], requires_grad=True))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(block.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: False\n",
       "    lr: 0.05\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам хватает знаний, чтобы закодить логистическую регрессию, которая отличает букву A на картинке от буквы B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing...\n",
      "found broken img: ./notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png [it's ok if &lt;10 images are broken]\n",
      "Done\n",
      "Train size  =        2808, test_size  = 937\n",
      "Train shape = (2808, 784), test_shape = (937, 784)\n"
     ]
    }
   ],
   "source": [
    "# если клетка падает, то просто перезапусти\n",
    "from notmnist import load_notmnist\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_notmnist(letters='AB')\n",
    "X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])\n",
    "\n",
    "print(f\"Train size  = {len(X_train):11}, test_size  = {len(X_test)}\")\n",
    "print(f\"Train shape = {X_train.shape}, test_shape = {X_test.shape:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAycAAADdCAYAAAChSA1LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6cElEQVR4nO3deXhU5fUH8O+dSTJJyMaaxRCIiIqiUCEExCooFXFfa62tYFVcgi3ys3UHpSqtWuqGWq2CVi1KLVC1UhURZVUCuEERkSUICYtkJ5PMzPv7gxqN73mde8lM5k7y/TxP/sjJycw7k3vunTeTc2IppRSIiIiIiIhizBPrBRAREREREQHcnBARERERkUtwc0JERERERK7AzQkREREREbkCNydEREREROQK3JwQEREREZErcHNCRERERESuwM0JERERERG5AjcnRERERETkCtycEBERERGRK3BzQkRERERErsDNSQdWW1uLKVOm4LTTTkOXLl1gWRZmzZoV62URuY7f78dNN92EvLw8pKSkoLi4GG+99Vasl0XkGryeENnD60l43Jx0YHv27MHUqVOxfv16DBgwINbLIXKtcePGYfr06bj00kvx0EMPwev14vTTT8eSJUtivTQiV+D1hMgeXk/Cs5RSKtaLoNjw+/3Yt28fcnJysGrVKhQVFWHmzJkYN25crJdG5BoffPABiouLcf/99+PGG28EADQ0NKB///7o0aMHli1bFuMVEsUerydE4fF6Yg/fOYmh008/Hb1799biSikcd9xx+PGPfxzV+/f5fMjJyYnqfRC1Vqzr5B//+Ae8Xi/Gjx/fHEtOTsYVV1yB5cuXo6ysLKr3T2RHrOuE1xOKB7GuE15P7EmI9QI6sqKiIrzxxhvYt28fOnfu3ByfPXs21qxZY/stvqamJlRVVdnK7dKlCzwe7kkpfsS6TtasWYPDDz8cGRkZLXKGDBkCAFi7di169uxp63aJoiXWdUIUD2JdJ7ye2MPNSQwNHjwYwIGD9eSTTwZw4IC/4447cNZZZ2H48OG2bmfp0qUYOXKkrdzNmzeLvzUgcqtY18nOnTuRm5ur5XwT27Fjh63bJIqmWNcJUTyIdZ3wemIPNycxVFRUBABYvXp1c5E8+eST2Lx5M+bNm2f7dgYMGGB70gPfdqd4E+s62b9/P3w+n5aTnJzc/HWiWIt1nRDFg1jXCa8n9nBzEkM5OTk45JBDsGbNGgBAXV0dfv/73+MXv/gF+vfv35y3e/dujBs3Du+++y7y8/Px2GOP4ZRTTmn+eufOnTFq1Kg2Xz9RW7BbJ48//jieeuopfPLJJ7jttttw5513tridg62TlJQU+P1+Ld7Q0ND8daJYs1sn31i+fDmGDx+OqVOn4vbbb2+O83pC7ZndOlm7di1KSkrwySefoFu3brj11ltx5ZVXNn+d15Po4uYkxoqKipqLZPr06di3bx+mTp3aIqekpAQ5OTnYvXs33n77bfz0pz/Fxo0b0aVLFwBAY2Mjvv76a1v31717d3i93sg+CKIos1Mnubm5uPPOO/Hiiy+Kt3GwdZKbm4uvvvpKy9m5cycAIC8vz/bjIIomO3UCAKFQCDfccEPzb5G/i9cTau/s1Mkvf/lLXHTRRXj//fexdu1anHTSSRg+fDj69esHgNeTaOPmJMaKiorwr3/9C9u2bcMDDzyAa6+9Fr169Wr+em1tLebNm4cvv/wSqampOPvss3HMMcdg/vz5uPzyywEAy5Yt498IU7sWrk4A4NxzzwUA/Pvf/xZv42DrZODAgVi0aBGqq6tbNDGuXLmy+etEbmCnToADf8ZSXFwsNvTyekLtnZ062bJlCy655BJ4PB4cd9xx6NevH/773/82b054PYkubk5ibPDgwQiFQvj5z38OpRRuu+22Fl/fuHEj0tLSkJ+f3xw75phj8NlnnzV/zr8RpvYuXJ3YcbB1cuGFF+KBBx7Ak08+2TyX3u/3Y+bMmSguLuZkFXINO3Wyd+9ePPjgg1ixYgUmTpyofZ3XE2rv7NTJ9ddfj+effx533HEHVq9ejW3btmHo0KHNX+f1JLq4OYmxbyZHLF26FHfeeSe6d+/e4uu1tbXayLmMjAzs3bu3+fPW/I3wo48+isrKyuYJEa+++iq2b98O4EBxZmZmHtTtEkVSuDqx42DrpLi4GBdddBFuueUW7Nq1C4cddhieffZZbNmyBU8//bTj2yOKFjt1ctttt2HixInIysoSb4PXE2rv7NTJmDFjcNlll+Gee+4BADz99NMtpmzxehJlimKud+/eqnv37qqmpkb72urVq1Xnzp1bxCZMmKD+7//+LyL33atXLwVA/Ni8eXNE7oMoEn6oTr7r6quvVlOmTInofe/fv1/deOONKicnR/l8PlVUVKQWLFgQ0fsgioRw15PjjjtOBQIBpZRSY8eOVb///e8jdt+8nlC8+KE62bt3r0pPT1dz5sxRgUBAffTRRyo3N1eVlpZG5L55PQmP75zE2JdffomysjJMnz4daWlp2tf79u2L2tpafPXVVzjkkEMAAJ9++ikuu+yyiNz/li1bInI7RNEUrk6iLTk5Gffffz/uv//+Nr9vIrvC1cnixYuxYcOG5mtJVVUVEhISsGnTJsycObPV98/rCcWDcHWyadMmdOrUCRdeeCEA4Nhjj8Xxxx+PxYsX47jjjmv1/fN6Ep6llFKxXkRHdvHFF6O0tBTr1q1DUlKSmHPRRRchMzMTjzzyCBYuXIixY8e2mNZF1N7ZqZNAIIBAIIDrr78e2dnZuP3225GYmMhpQtRhhKuT+vp6VFdXN3/+m9/8BoWFhbj55puNf+ZF1N6Eq5OqqioUFBTgueeew9lnn43169dj5MiReOGFFzhmu43wnZMYqKysxBtvvIF3330Xc+bMwRtvvGF8wQUAjz32GMaOHYuuXbsiPz8fL730Ejcm1O45rZO7774bd911V/Pn99xzD2bOnIlx48a1wWqJYsNJnaSmpiI1NbX585SUFKSlpXFjQu2ekzrJzMzEyy+/jJtuugm/+MUv0KVLF0yaNIkbkzbEd05i4JVXXsGFF16I/Px8TJ48GVdddVWsl0TkOqwTovBYJ0ThsU7iCzcnRERERETkCp5YL4CIiIiIiAjg5oSIiIiIiFwiag3xM2bMwP3334/y8nIMGDAAjzzyCIYMGRL2+0KhEHbs2IH09HRYlhWt5VEHp5RCTU0N8vLy4PHEbo9+sHUCsFaobbihVlgn5HasE6LwbNdJNP55yuzZs1VSUpJ65pln1GeffaauuuoqlZWVpSoqKsJ+b1lZmfGfOPGDH5H+KCsri0YJ2NKaOlGKtcKPtv2IVa2wTvgRTx+sE37wI/xHuDqJSkN8cXExioqK8OijjwI4sCPv2bMnrr/+etx8880/+L1VVVXIysrCCTgdCUg8+EVIO3/TQxVyhy3bL6b+rutGLdakgmJuotV2/18hqEJi3Gu17W9wygO1WuzN+kPF3Nd3H6vFPl/VS8zt8/xeLRbcsEnMtRL0NwRVsOXPKKCasASvo7KyEpmZmeLtRFtr6gSIYK20lsdwnIf0ugieOEBM/ddfn7d9d9KxHs3jXKpvU22/Xa+Ppnzi1BPE3OCu3XrQ9BvLGM4tCaAJS/DvmNVKm9eJ9DMwHV/CMW406CgttO3Xcuptx7yhxc7qtE/MbcvrjEl9qFGL7RViAPCJv7sW+09lfzH3nY+O1mJ9Z9aLueqj9XrQwbmptTpcnVB8kmrCUA/eLp21WNkjOWLuokH6Nfy0jy/WYsF6P9aNezRsnUT8z7oaGxtRWlqKW265pTnm8XgwatQoLF++XMv3+/3w+/3Nn9fU1PxvYYlIsCK8OYH9zUlyWkBMzUjXL1JNhhcOiW24MQgaHlpbb07qAvr9pXjkwyxReCHnSU4WcxO8Pi1mGY4PyxI2J9LzoBCzt6+d1gkQxVppLdOLI+E5txLkn69UVybSsR7dzYl+h6baThX+4WOCR56lLx6/xuMxdpuTb+46FrUSkzpxsjlxctwJx743VcgDkJquH0cZneT7asvrjElCSF9DoxADgNQk/bElBeQa8aToz1mCV34hpcR6sn9uarWOVicUn6SaMNSDV7h2eVP112KAfA035QLh6yTiFbpnzx4Eg0FkZ2e3iGdnZ6O8vFzLnzZtGjIzM5s/evbsGeklEbmO0zoBWCvU8bBOiMJjnVB7E/Nfudxyyy2oqqpq/igrK4v1kohcibVCFB7rhCg81gm5WcT/rKtbt27wer2oqKhoEa+oqEBOjv63aj6fDz6f+a2ftuDNytJiMz8aKOYuyj1ciyV65LeZr+u5SIud3Un+e1knf0fvJPfBfb3F+Aub9QkejQH5LfC0ZL8W65O5R8y9Mvs9LXZFpvybm3EZ/9Zi3r7y41j/U/15O3/mjWJuwe9XinE3cVongDtqRWJ55LdnpTaorafZX79fNYlxXxv/yYH0N/2mPrPTUvVamTpS7rlKn71Li1nCn4UBgArIf2ba3sWkTqQ/0zX8vL2H99Fi6yd2FXNfO/0hLXZ0UoqDhbW+t0Q6bkOQ+xUlHsPvM1OFP/8oMPw5Y0GCfi4/u5P8p0fI1+MLRsk/2/uu+6UWS3xzlZgr9iXGcY21p+sJRYip30q4MHvS08XUtPn6tf3TQ18w3KFe7/u26T0rof0Nhu//3ppsZTmQlJSEQYMGYeHChd8uJhTCwoULMWzYsEjfHVFcYp0Qhcc6IQqPdULtTVT+z8mkSZMwduxYDB48GEOGDMGDDz6Iuro6XH755dG4O6K4xDohCo91QhQe64Tak6hsTi6++GLs3r0bkydPRnl5OQYOHIgFCxZozVpEHRnrhCg81glReKwTak+i9h/iJ0yYgAkTJkTr5onaBdYJUXisE6LwWCfUXsR8WhcREREREREQxXdOYs7Bf1QO7tP/8+5hv5T/G6/0D7qU4b7+UvgTLVbwzsti7kBhaob0X3cBeTLKMSt/LubmnbdOjHex9P90byQ8vgohDQCmdRquxW46+xgx9+op/9Ri4zL0CUYA0EuYrrL+6sfE3MLc8Vrs8Gs+aBmI4X/cjmvS8e9gys3ok1dHcjUxY55wpE9I2XWWPsELANJnR3BBFJaVkNDiH7SqkOEcMET/r+Tp9+8QU2/Pf1GLSefyA/TJXKaJdAnCcRSJfzQq/zf51k8BkyZIOhEy/JNR6fk5zfCPKxMfn6nF/jTmXDE3+PkmLSZN8ALie4oXdRDCddk8RVM/j+x/RZ4w+Mah87RYVWi/mJvp0c9vnbbo55ag3975hu+cEBERERGRK3BzQkRERERErsDNCRERERERuQI3J0RERERE5ArttyG+lUzNcRCaElWT3Liuauu1WBByk5JEbl6UdXvC0CVo4BGaNlXQ0NQoNDuqYFBMDe1v0GIZf18h5v5t11la7JLnnhBzpSEApmbSzWc/qcWGrLi2xefBxgbghfni99MPkJpylXwsYIg+COFPec8YbjhRi0hNwW7hcfB7ndsHvS7GX+6sN15LwzkAiA2PHOrgjAoEoKTn8Xs+v0I/N27u87YhW8910uTus/Tj3qkmof5M144ZlT212KK9R4i5nZP0xteB6dvE3JKssh9aYgtS87xpvVLcNCjmlBT9GjH3he1i7pdj9Abg4J69Yi48wtpChnMeUTQZzl9Wgn4eMb0u3fTiQC32Rf9ZYm5tSH89l2aZBn7oMrbqtR5osjc8g++cEBERERGRK3BzQkRERERErsDNCRERERERuQI3J0RERERE5ArcnBARERERkStwWpeBCgTEuHGKl6Dy5D5abJBPnygCyBNeTJNcZtd01mIpH2wSc00zRUJ+vx6MxPQfYZiElSg/5oSFpVrsqm2niLnP9XpPi0lTagD5eet95ectv7euEWtfEL+dfoDl1SfXKMPkmq1npmkx0zHt5Ph3A9N0IWkS0biMXWLu0yPP02Kp/1wp5orPu+EcRbKE/DwkeL6dNPPF1frkKgBAk348H/qPq8XUjRc8bvv+vdKkuwiQasd0fD71hD4hMfvhZWJujRArS8wVc5+/8Awt9sA9j4m5Q4VhP6ZzufQ4pMmNgPw8PHqIXE8/nz9Si1WOyRBzg9XVelCcmmQBHKBHUSRN5QLkyVxfPDhUzN00Qp+IWhXSJ/MBQKYnxfba9gX16bSdtuu3GwjoE8AkfOeEiIiIiIhcgZsTIiIiIiJyBW5OiIiIiIjIFbg5ISIiIiIiV2BDvFMOmhp3DZaa5mT1IaEh2Cs3Pz2x9SQ9d98WMdfUwB+1ZlqpOVpo5jX5cHsv+QuGsF2T8t5s8XldTQj/at1NdkgqoB+nJgNP2RDFlbiTX+l1lWrJDbzbT9dr5fB/Gm44Ss3UHUooBODbgQW5y+RzoO/fH2qxfeOGibneC/Wfiz9kOLfavxxETTBZCHrk87MnWe9cDzUIg1QAZPx9hRYr6T5BzF17s94oHxCa2Q+wf+2QBmjUh/RGYQB4sXCRFhs990wx1zpdf8yqSfoZe9gQTxEjDRKSGt8BoOy247XYpp/KAylqQ3pDuqnxfVugVosVJOiDbgDgo0Y9nrhjnxazQvI55Pt4xSMiIiIiIlfg5oSIiIiIiFyBmxMiIiIiInIFbk6IiIiIiMgVuDkhIiIiIiJX4LQuh1RQmEZlMHT4+qisoWJpnhYrwBY5Oc6m/Hi9ofBJ/+NxsLful9RyykV1kv376ZAME3ykaWyeAf3E1EcL/ipEO4m5CQ6m8rhZomX/cUwa9pYW+3dyrpgbatAnrMAyjH9SHBkkCewoB74z0cn31Q45UXheg/LAtfgjHRrShEWYJlLJpKmQh8zdJuZuv1GfAJRvmAAUVPp52uvgmpbqkX9w0hSv//R7Tcz90VXXabEejy7TE5X91wZE35CmcgHyZK494+WpgZ9c96gWqzVMqkvz6CP7Ztd0FnPvekE/9tdfI08BW9tQoMWC2/VzbNA4ma+l+HrlSkRERERE7RY3J0RERERE5ArcnBARERERkStwc0JERERERK7AhngTU7Op0DyYcGhvMfWG3JeFqNz8lCE0KZlkf2ivoQgAIDQUtjUnQwTysyqjsoaPGlNafF7XGPvnxc0sr9zYrYTjf9vpcjNdN6/e/O43NMP5vtOoHM88MJw3BNd33qrFXjnlVDHX9/qHWsz4MwrYb2TuUCyrxXnd4/OJadLwAYuniwMM1xMV0DvtVaPckLtkf08t9rP0fWJuSOjgj9bojCZDQ3vt8fVarIfef0wUljQ4Qmp8B4D684u12DuTp4u5IeF1pemaujOgD6R45lfjxNzUo+0PV3lvb18tpgK79Ziyd33iOydEREREROQK3JwQEREREZErcHNCRERERESuwM0JERERERG5AjcnRERERETkCpzWZeBkEs7XxTli7iCfPkGhPiRPZkj16Llv1svTFlKXfa7FTPOwnEzKighpypkw4QkAvFmZWuzXBW9HekUAgHu3nNHi80CdH8ATUbmv9sDJcVM4enMUV2LPvqA+UQcAXqw5QouVZJWJuUHDJCKvZf93OFKuk5rfNka+r76vC0EH6yIASgHfmf6kghzB5ZjpmBNO+1aifP0qTpbqL03MdTL9LloC1fKETSITaSoXIL9+DJ0wUMx95E8Pa7E0S54w6BcmYIUgn9/OmfxbLdZ56XIx9+tLh4hxySfbD9Fih0Kf1mUXr25EREREROQK3JwQEREREZErcHNCRERERESuwM0JERERERG5AhviTRw0m5afZL+xUmpcAoBU6E13d31xlpibVvmlHvTIDfymZvRo8fj0hq1QQ4OYu+uio7TYaamLxdwmpT8Or9R8b7Dr5YIWnwcb5TV1SNKxYxpi0K+vFvtj7+cMN5yiRRJgOE5bqc7QzP7AktO0WMmZT4m5ThrfnUi07D/mX50oH//vI1mLqSa50Z7IRAmHopUoN3xbyULzbaN8zCm/X4vtOrVAyAQKE/Xmd+n8DjirHYlpyIWT+8pbGPumfHIvqfldanwH5OtnycyXxdyBwmup2pD8uiXNo18fBvzxOjE3Z9YyPWh4/Zjcfb8Yl6iv9Ot9a/CdEyIiIiIicgVuToiIiIiIyBW4OSEiIiIiIlfg5oSIiIiIiFzB8ebkvffew1lnnYW8vDxYloV58+a1+LpSCpMnT0Zubi5SUlIwatQobNy4MVLrJYoLrBOi8FgnROGxTqijcTytq66uDgMGDMCvfvUrnH/++drX77vvPjz88MN49tlnUVhYiDvuuAOjR4/GunXrkJysTxRwBWHqk5NJOKN+9JntXI+DCVN7SrPFeBr0aV1WovyjVE227w6Wx7A2aYqRYQqKNJkrobc8tWX8pPm211YlTKno5u0k5g5de6EW6/7E8hafB5w8MQchnupE+rmbhtx8Nbq7Fjs6SZ7S4ReeY5+VaHtdpkk70lStjxq7ibmHvKnn7hpTJ+b2MBxP0jqcTPbywH7N39RVPpcsOmW8FktYWGq4Q/vT12ItnuqkPUis1mOma52Ta2DohIFa7I5bnhVznUzQcvL9AejHuGk6YKpHn1A24INLxNycV1bpQanGVAho3UMzYp24gGGilTSZKyE3R8wd/tLHWuzsTvViblVIn5SV6ZGvtUc+pU/m6vWQMJULgCVMAZOm7QHA0Tk7xbik047ITrVzvDkZM2YMxowZI35NKYUHH3wQt99+O8455xwAwHPPPYfs7GzMmzcPP/vZz1q3WqI4wTohCo91QhQe64Q6moj2nGzevBnl5eUYNWpUcywzMxPFxcVYvny5+D1+vx/V1dUtPojas4OpE4C1Qh0L64QoPNYJtUcR3ZyUl5cDALKzW/45UnZ2dvPXvm/atGnIzMxs/ujZs2ckl0TkOgdTJwBrhToW1glReKwTao9iPq3rlltuQVVVVfNHWVlZrJdE5EqsFaLwWCdE4bFOyM0c95z8kJycA01AFRUVyM3NbY5XVFRg4MCB4vf4fD74hAadNiU2fMsNpN6+h2qxO3KfM9xwmh6x7D/WQxbZb0hUTXpTFgBHjbDGPkWpiV8pMbXp1MFa7McPLBFzx2fu0GL1IfkxS83v524cLeZ2uWyfFnNTO/DB1AngjlrJHGO/Qa61pAZXAPAKv1P5W8XxYm76ax9psY33yU2FPeR+x1YzNc87GRiwbbTewHvoQvn+nAw4cLN4rpO25GTIRJ9LPtdipT8qEnO9qfo1pTBnj5j7XN9HtVhugn79A4CgcOlItOwXn6mepPOC6Xpy9OslWuzIifIwipDd4olRkbFOosDBUBFvt65aLOVl+fXYrd02aLF9QbkhvrM3VYsd9vdrxNw+U/TmdytRv2YAhuZ3Q7P/yK76eqXrFgB02hHZ4z+i75wUFhYiJycHCxd+e9Wsrq7GypUrMWzYsEjeFVHcYp0Qhcc6IQqPdULtkeN3Tmpra/HFF180f75582asXbsWXbp0QUFBASZOnIi7774bffv2bR5pl5eXh3PPPTeS6yZyNdYJUXisE6LwWCfU0TjenKxatQojR45s/nzSpEkAgLFjx2LWrFn43e9+h7q6OowfPx6VlZU44YQTsGDBAs7apg6FdUIUHuuEKDzWCXU0jjcnI0aMgDL0GwCAZVmYOnUqpk6d2qqFEcUz1glReKwTovBYJ9TRxHxaFxERERERERDhaV3xysl0m10jsrVYgWEqiTQpJNUjT1B4vV5/+zV5y9dirkpP12KW4e1bq5M8mSiUqU+/qjksQ8z9+ih9kkP/0foUBwB4prc+tSXNY/+t5SUN+roA4Dd/u0qL9fpDqZgrTqPQJo5ZgPkXUe2TNHUNgArok0USDu0t5j5w+MtCVJ4WlIAojb8SfLC6rxjv27BSi03bdoaY+9rhb4hxaWqYNBkoms445UMttt6Qq4Jumk1H0eZk0tU/+rytB/tEYhX6NTBouIhK07ZMuZK/1eSI8bveO0eLHf6McC0AcPgKvZ5ChvOjSHoX4wfe2SCXMkypkl4Aeo49Uky9bM5/tNjP0vWJoSbSVC4A6PfEdVqsz1R9KhcAWAn6S3kn1wFvn15ivCRLeo0lX+8zvqzT12B7BTq+c0JERERERK7AzQkREREREbkCNydEREREROQK3JwQEREREZErsCEeAIQGPZO9Q5uisoQzUhv02Htzo3JfTknNilJTIwA8WXWoFpu2+Ewxt/c8vV3K9/YaMbcgoDeCKVMzm9TY+P1mxQ7YvGh55edLaogv/0mumDvEpzfDNSm58c5Jo67E4+B3J7nv229mXfeR3PyHw+VwUDpWHPTOmjgZGHBvtn78nzf0Sjl5xcd6zFQrITbPdyR+pV+/TPUbdNDOmigcyz6r9S8vpOvM6lq5frus0u8vYfsOMVc/48F8TZBqx/CckYtJrwscnP+2n9pFjE/9RB+wcmudPPioU4b+Os/zbpaYW/CQfs6XGt+BCAxBMQyDOOzdcVos1ChfS47csFGLtWZVfOeEiIiIiIhcgZsTIiIiIiJyBW5OiIiIiIjIFbg5ISIiIiIiV+DmhIiIiIiIXIHTugCopkbbuZcNXh6VNVy+7cdabMmXfcTcJJ8+cUUpedqCZckTSJKT9NvITa8Rc8f0+FSLXZ6xScy9ImO7Fqs6YaGY+3jyCC1W2DRAzE1YWCrGRR1wElekBcZU2s6NxLQu6TZM37+g3qfFspaVibnSVJ7cpYZFXCSHIzF1SCJNIpKmKQFAqkef/rLlzE5ibu8Vesw4qY3TujoUn6VP25Ni0eRk+qPk4bwP5S/cqcc/vlWfjgQA5y+7Rov1/aNfzA19tF6LSVOTLKUMY8CoTRkmT1kJ+nHuSZPPoU3/SNNib/S9T8z9qLGbFsvy1Iu5414u0WKHClO5AMBK1M/5Tl6rGid7CdM5K4/rLua+NOxhLfZQ+Sgxd9d+udYOFt85ISIiIiIiV+DmhIiIiIiIXIGbEyIiIiIicgVuToiIiIiIyBU6VkO8x9CgKzSFegYeJaZe1flJIao3TwHOGmm3TDlCix36n1VirtToJDU5OSW34gKvJR+ixeYVy01RtTdXa7HlA14Rcyf+5HMtlniq/DMqfONKLXbE1WvFXKHfUmiSs4D23DcvNAWajhFvdg8t9tAxL9m+q0g0jIcg/dDkY+HPW3+iB7frgxhMslaVi/H1jXITY7+kVC0mNfUCzhp7W2vQyP+K8b1CTAVM1U3xrj6kN8lKAxQA4NA5eiP4oXPlJtuGrnoDcU1Pw2CFEfu0WGnR82Kuk0EZEtMADukccmxSspj7xYhZWmzT8Fox9+K7f6vFuj6lD8ZRit3wbiA1vgNyM/nucweJucMy9ddet351upjbJ3WPFnutrL+Y2/eP+jk7JDS+A217zraC8ouhZ/cO12LLDIOa+jStieia+M4JERERERG5AjcnRERERETkCtycEBERERGRK3BzQkRERERErsDNCRERERERuUKHmtZlefQJRoA83an8+CwxNz9Bn8xVG2oQc9M8+qSQeXXyZK+UDzZpsaBpuphXj1tOpwQJD1qF5IkNoQb98XneWyvmZizWb6PfnF+KueuH/02L7QvKE5M2j/mrFusz63Ix97BfCFMjtOfH066ndVnCMWKa1rVnjD59Y0TKm2KuNCmntdN3ACCohB+GXK7YurSnFusFeVqXp1MnLRb4couY++CuU8T4X/L1yTwByBODvK38fU+CYUKZZEbB62L85wOu0GKhj9bLNyKdY4TpheRe8qQ7WcYm/fj0vrtazNUrR44BAB7UQydcNkFMffPe6VoszfKJudL0O/P5Ro+bpur5hclafRLla/OCyQ9osZ9u/rUWCwQagEXzDWujaLB8+nGj/H4xd/O9w7TYqT+Rj/31VTlaLD1Rfp03e+4ILVZw5zIxty3PrKbXc5KaArmmttdnabGEL1Js3640WdZSCrAx2I7vnBARERERkStwc0JERERERK7AzQkREREREbkCNydEREREROQKHaoh3om6E2tt5zYZmu4kf/xitBjP2Kc3xIvNqjA3fEWNpXcmW0lJYqq0tp5/lvfA9cMatVhnb6qcG9JzN508U8wdccZVWsz3+octA0Jjd0dVOVoeQiBpbUO8qUE11aMfT6ZBEz3fkuPyHdr/Ob9Zeoz8BaEhPlqkBmAA8KsmLWaqlW1ndNZi+R/J9ycOTmBDfLsVEHpZpaZVQD7Hq0b9PAxAHNKS9ZxcN0VjrtFin5/0rJgrHfc+K1Feg7QsQz2lWvpjk64xANDNq48BUDft0WN1fmCR7aWRA6ZjVHq90TRqkJj71589rsW6evaLuYfl6fe30i8fd/e+rjfPw7ReqUk9SudbJwOgavrJx/7DvedqsR/3mORkEULQ3nsifOeEiIiIiIhcgZsTIiIiIiJyBW5OiIiIiIjIFbg5ISIiIiIiV+DmhIiIiIiIXKH9TusSJkypQEBOFSYrjDtqpe278gj3ZVK1LFuMZ0Cf1iVN0gFiME1H6RMmjFNbBN7VG8T43LpcLXZp+l4xNwT7E9G2nqX/PA5/3fa3xx/L0o536Vj3ZmSI337f4Fds35WTyVyt9aVcrthzrD5yyHvEMDE3JJzhkmqFiSkAkKxPBjLxuPj3OoWnbtZiTffKuSpg/zFT/LOE06jpuihN2jFeQ6WpPIZpk2mLhSlzJ8lLaEs+S345JE0YfOCwOVqstiaEkyO+qo5Hej0mTrkCsOu647VYw8gaMfeqDy+zvYakVWlaLP+JT8RcVSPHY005mFSZuFueRDZqxbVarNerhuunzTUom5NS3XuFJSIiIiKiDoWbEyIiIiIicgVuToiIiIiIyBW4OSEiIiIiIldoxw3xwr7L0IijjuunxX7bdZbhhvUmv0yP3qALyI10eUsaDLcrrMtBQ1ObE5rkTUJ+vxhvCMlNWK11WN+dUbldt7K8Xljfa1SXGlcrTz9K/P5zO72nxaRjF2h9Q7xXqkuDY5OSxfia2x5r1RoioS0HAwBAgnDeMXmoUG/Wvb7f5WJucP1GPfj9RmYVgoN5FNQRSecLw+CWjG2GBnxBWw6eMJ2bmoTXDUck6o+3WoiRmdT4DsjXLjV8oJh7xvj3tVh+0tdibrpXf+212d9dzF3yh0FaLFQjN9qLDfymIRPRIg1lMr1GE3J7/KhCTL281zIt9kync8Rcn3RXHv2+LGXZup7wnRMiIiIiInIFbk6IiIiIiMgVuDkhIiIiIiJX4OaEiIiIiIhcwdHmZNq0aSgqKkJ6ejp69OiBc889Fxs2tPzv3w0NDSgpKUHXrl2RlpaGCy64ABUVcrMNUXvEOiEKj3VCZA9rhToaR9O6Fi9ejJKSEhQVFSEQCODWW2/FqaeeinXr1qFTp04AgBtuuAGvv/465syZg8zMTEyYMAHnn38+li5dGpUHYGJ59ek2yjA9ZMdJ6VrMNI2nNqRPfEjzyFOFXqnrrMWSSr8Qc8WVGSYmuYI0HQIQJ0QkHJInpvb1rYnkipo1BfWfnTRJIlrcWiflpzXZzg3IRyS8LniztT7UGJXb9RqOaZ8VnalyTkiThPxK/nn2SUzTYtvHyFNpcoVpXd8/d1pRmtbV5nViWS3OW1aiYVpQU3SOL3IPaRrhfiX/3KXr+/iyEVqssbYRwAutXZrIrdcU24Rzq2milWeAPj21Ok++gi+9qVi/XWFCFACkfKVP2wp9/F8xF/hMDxmuD20+mUtchP3pqR6f/lwmTe8i5j5vnanFMt7/RMyVLhHi5DVl7/lytDlZsGBBi89nzZqFHj16oLS0FCeeeCKqqqrw9NNP48UXX8TJJ58MAJg5cyb69euHFStWYOjQoU7ujigusU6IwmOdENnDWqGOplW/Bq2qqgIAdOlyYNdVWlqKpqYmjBo1qjnnyCOPREFBAZYvXy7eht/vR3V1dYsPovYkEnUCsFaofWOdENnD117U3h305iQUCmHixIkYPnw4+vfvDwAoLy9HUlISsrKyWuRmZ2ejvLxcvJ1p06YhMzOz+aNnz54HuyQi14lUnQCsFWq/WCdE9vC1F3UEB705KSkpwaefforZs2e3agG33HILqqqqmj/KyspadXtEbhKpOgFYK9R+sU6I7OFrL+oIHPWcfGPChAl47bXX8N577yE/P785npOTg8bGRlRWVrbYwVdUVCAnJ0e8LZ/PB5/QoNNqTprJj6+0ndrk4Hb/uGG0FutW/bmYayXoPwpXNFoZSAMHAHnN1UX5QiZwojBHQGpUBACPg3301q168+/h2GL7+yMlknUCmGtFBQJQ32vW86SmanmTh75qe+1Onu+2lupJivUS4k6X03bIX5iuh1Sw5TAEpeThCJHSVnVyoGn028bRUH297TUq95aDK4jXr6B83Ow7wv6AiZDYZitfeySm64k0YCLNkgfb3L3nSC22s6RAiwWCftvrOlhuf+0lHQeA/LrAM/AoMffqOf/SYiNTdou5aZa+/k2B/WLulb++QYulfCI3uYsDlVz8egweoSYMA6BwWG8t9MbMx8XUHQH9mC45/Qr5dj/boMekddkcsOLolKuUwoQJEzB37ly88847KCwsbPH1QYMGITExEQsXLmyObdiwAdu2bcOwYcOc3BVR3GKdEIXHOiGyh7VCHY2jd05KSkrw4osvYv78+UhPT2/+W8bMzEykpKQgMzMTV1xxBSZNmoQuXbogIyMD119/PYYNG8ZpEdRhsE6IwmOdENnDWqGOxtHm5PHHD7z1M2LEiBbxmTNnYty4cQCAP//5z/B4PLjgggvg9/sxevRoPPbYYxFZLFE8YJ0Qhcc6IbKHtUIdjaPNibLxj16Sk5MxY8YMzJgx46AXRRTPWCdE4bFOiOxhrVBHwzY/IiIiIiJyhYOa1uUqljxtQZwOIUwwAoDf9nvT9t0lClM+TBqWdxOi8rSuuOPgedh9if2pOAHIEya8hp+zJGeR/Wku7VX9yf212LiMZWKuNNEm0Wrb59A0VcetpGk/bS3BwdSiP/V9WYxPPvSnWizw5ZaWgShP62or3n594fV+O91n1/FdxLyuT+n/tM4b/UFMbUM6jUoTdQBYXv0YVwH5PBxqaLC9hGMuWmc714km4Tg1ncd2Beu02ND5k8TcI+/6Uoup3Z/pMdUUbontnmmilSVMBbOa5PPKo1fp56THG+xPyvJulv+vS0rFB7Zvw9WTuSSmyVySrV9pobMuvFJM9e7Xj+nQuvX270tal83rSeyvsERERERERODmhIiIiIiIXIKbEyIiIiIicgVuToiIiIiIyBXaQUO8YX8lNN00FR8ppl6S/r4QlRvpUqwkuytD3tL9tnNVKPyowIgSGswtr6HBVogrv9whWn9+sRb7ZLhp1rp+u1JTIwCkeZK12K93FIm5GbM/NNxfx7HtDPu50hACbxv/3sINDebxxvSc+YXG3EE++by189RcLdb9iS2tWpdbWdW1sDzfPjd7h6aLeXuHDtaDfvsNp06Gd7Q1r9S3bmimDTXYf8xW0TFarOxmecjFf3q/oMVMAzF8VqIWM10jpOb3V2ozxNxHf32VFuu7YKWYK96bNERAhYD4muthj3Fggh73HF4oZALZT+/QYsUZS8TcPQG9Lnf4s8TcLybqr+mCFbvEXCtBf7kbd43vBk4e2+6L9WE5l096Tcx9bP2JWiz/AsNrVem8Z2MEtglfERARERERkStwc0JERERERK7AzQkREREREbkCNydEREREROQK3JwQEREREZEruHZal5WQAMuysTzDhCklTFf56iR94hMgT/moCsmTtjI9KVpsVnUPMTdh9RdaLGSY5GIlCo81ZJieJTFMO1FBw8QVYYqCcWJYQJ/M1XSqMNEGwD0P/EWLSc8vID/H0vMLAJubarXYZzcdK+YmhEr14PcnjrSjySqWzwfre1NtLh223Pb3e6L0OwppapQ0fQcAChdcqcWOfLhezG3sqh8jVtDBVBDDMCWrST4gGrP0SVePPfqQmHt0kr420yQiN0woC55WqQefaPNltInAVzuB7xx/h1/xlZi3c14/LfbxkL+LubUhffyV6RiPFif3d9U1r2qxhRfKUyw7J+nn58LUPWLu+M76QdPD20nMlerBVAtSbshw4t4e0Nf7l8vHiblJS/WJjpbPJ+aqxkY9KE04M0wRi3uGaW5KiAc/2yDmVozprMXmN/YWc61c/fWUtV+eDmp9tVYIyif49jKZS+LksXX9q/7a4PWX5SlrvVCmxYxHeSsmc0lif3UkIiIiIiICNydEREREROQS3JwQEREREZErcHNCRERERESu4NqGeBUIQH2/sUlqdDI0AiXkZGuxcRe8Zfv+E2G/GT3RkluELJ/eSIsauWlIbLqLcIPRd0nNf6FBcmPkxsv03PdPny7m5npTtZjf0CgoNb+X+oXnAcCkX0/SYskLPxBzrQT9sNYaxtpR86L/x0chmNBy2MOU7vpgAhiOadPAgtZy0mjfc76eG1q7TsxNtPPzjSBpjMaMKSPF3McOWaHFAoYWQm+UfjeU4ODcdW//uVrssZ6jWgZCfmB7a1cVe1ZiUovBEaZhISn/zNKDQ+TbTPPIQ1YkcnO3fI73CFMbTE3jTuq3JEtvcJVizunN79JADEA+Pk25QeEamOoRrqsAfvLMb7RYwdJlYq50/VN+uem6o/n+MCJv925i3vbH9Sb3wTnysbTXr//MGoL6awUAaJqq36733dVirq1rfUfw/YE/gHGQweZ7h2mxISPWi7mfvHSUFst5yFBTEf5Z8J0TIiIiIiJyBW5OiIiIiIjIFbg5ISIiIiIiV+DmhIiIiIiIXIGbEyIiIiIicgXXTuvafE8RPMktJ6EcOXirlpeaIE93ui5Xn8w1IkWflgLIU1RME0Gk3EvT94q5De9/psWe2Xq8mFvboE8PCYYM01m8+hSGTj75echPrxTjp3X9VItdkr5EzPV9Z8JN89qUPGlDmiizL7hfzC0qvUyL5f9WnpiSvFGYzCVNqEDHm9ax7UwPPCktn3dpgo9pIo7083VCqgnTGt7dLx/T6Wt3ajFHP0XDseDoJpLk5yHU0KDF3vikv3wjwrQuaeIQAAgDmSJCqkHTz+iMVP2xTR7Ts+X3NjYAMyOztlhSTY1QVvgJiF1e+ViLjay4UswtL9avE+edJ59H783Wbzc6c/LMmoQphSHIx4bENIFPqnUn5xXj5DqhRoZ/fL6YWjB1pf7twgQhwDAdkwDok1KDu/eIeenP99JiG9TRYm5juv7z9TbKtZj1gV4nIWlSKzretd7IcH6XHDq3VottXitPas3bsE+Lme7JNP3wYPGdEyIiIiIicgVuToiIiIiIyBW4OSEiIiIiIlfg5oSIiIiIiFzBtQ3xT531FDp9r4lqaLLedGdq9HTSFCrlmjjJvSKzXI8d+0/b39/WdgbkZvQP/Zla7OXdQ8TcZaVHaLHDXtSbbgEgd/lHWixoaHyTGhvZDHfA1cPeRXJa+FJOiFL7bQByI5zU5PpE+Qj5NraW6cE2Hniggvafn/TP5IEZOE0PeQ3HdFty8jNSZ37d8vN6f7toiLcrVFenxZL+s0rMLfiPHlv9xxQxd/Bl12qxnEu3iLl39ZqvxQb5DMecA1LjeiTa8ncF9eesLCA3xL9dqzdNzy0bIOZWfdBDi/X+w2oxNxTSj3GlDLVnGlJB2PurIfAmfTuMyN9Zfg59X+vPYShBzs375yYtFiivEHPtt3ZTMwfH874j07RYY7r8c8taqTfPt9XPh++cEBERERGRK3BzQkRERERErsDNCRERERERuQI3J0RERERE5Aqua4hX/2vsqavV226qm/SmHXOTux5zktsR1QTk56ferzcaNtXJ/2E3tF9vfg8E5IZ4S/yP5YaGeOFnp9TBN0YH0PS/24jfxshv1t5Qqz8P1Yn68xWt499vuF2fpcdNx01AOhZM//VW+C/XkWAZmmeVsLagXz6mq2v0NTt5fqLFyRqC9X7x83itlW/WHUAT0AYPwVLy7/yCjfoxY6qHWuE4qm50b6twTVBfW63hetJQJ9RTnTyMJdggXE+U4drj4HoSjYb4eL+mfLPu7x+nQb/8HAaF//AeCsm5gZD+MxPP+RR10nko2Gj6uel1af65Cbch1ILdOrGUyypp+/bt6NmzZ6yXQR1EWVkZ8vPzY72Mg8JaobYUr7XCOqG2xDohCi9cnbhucxIKhbBjxw6kp6ejpqYGPXv2RFlZGTIyMmK9tIiqrq7mY4shpRRqamqQl5cHjyc+/7rxm1pRSqGgoMDVz/fBiodj6WDFy2OL91phncS3eHls7aVO+NorfsXDY7NbJ677sy6Px9O8m7L+9/8BMjIyXPtEtxYfW+xkZur/vyWefFMr1dXVANz/fLcGH1tsxXOtsE7ah3h4bO2hTgC+9op3bn9sduok/rb3RERERETULnFzQkREREREruDqzYnP58OUKVPg8/livZSI42OjSGnPzzcfG0VKe36++dgoktrzc87HFh9c1xBPREREREQdk6vfOSEiIiIioo6DmxMiIiIiInIFbk6IiIiIiMgVuDkhIiIiIiJXcPXmZMaMGejduzeSk5NRXFyMDz74INZLcuy9997DWWedhby8PFiWhXnz5rX4ulIKkydPRm5uLlJSUjBq1Chs3LgxNot1YNq0aSgqKkJ6ejp69OiBc889Fxs2bGiR09DQgJKSEnTt2hVpaWm44IILUFFREaMVt1+sE3djrbgD68TdWCfuwDpxt45SJ67dnLz00kuYNGkSpkyZgtWrV2PAgAEYPXo0du3aFeulOVJXV4cBAwZgxowZ4tfvu+8+PPzww3jiiSewcuVKdOrUCaNHj0ZDQ0Mbr9SZxYsXo6SkBCtWrMBbb72FpqYmnHrqqairq2vOueGGG/Dqq69izpw5WLx4MXbs2IHzzz8/hqtuf1gn7q4TgLXiBqwT1gmFxzphnbiGcqkhQ4aokpKS5s+DwaDKy8tT06ZNi+GqWgeAmjt3bvPnoVBI5eTkqPvvv785VllZqXw+n/r73/8egxUevF27dikAavHixUqpA48jMTFRzZkzpzln/fr1CoBavnx5rJbZ7rBO4qtOlGKtxALrhHVC4bFOWCdu4cp3ThobG1FaWopRo0Y1xzweD0aNGoXly5fHcGWRtXnzZpSXl7d4nJmZmSguLo67x1lVVQUA6NKlCwCgtLQUTU1NLR7bkUceiYKCgrh7bG7FOom/OgFYK22NdcI6ofBYJ6wTN3Hl5mTPnj0IBoPIzs5uEc/OzkZ5eXmMVhV53zyWeH+coVAIEydOxPDhw9G/f38ABx5bUlISsrKyWuTG22NzM9ZJ/D1O1krbY53E3+NknbQ91kn8Pc72XCcJsV4Axb+SkhJ8+umnWLJkSayXQuRqrBWi8FgnROG15zpx5Tsn3bp1g9fr1aYLVFRUICcnJ0arirxvHks8P84JEybgtddew6JFi5Cfn98cz8nJQWNjIyorK1vkx9NjczvWSXw9TtZKbLBO4utxsk5ig3USX4+zvdeJKzcnSUlJGDRoEBYuXNgcC4VCWLhwIYYNGxbDlUVWYWEhcnJyWjzO6upqrFy50vWPUymFCRMmYO7cuXjnnXdQWFjY4uuDBg1CYmJii8e2YcMGbNu2zfWPLV6wTtxfJwBrJdZYJ6wTCo91wjpxldj245vNnj1b+Xw+NWvWLLVu3To1fvx4lZWVpcrLy2O9NEdqamrUmjVr1Jo1axQANX36dLVmzRq1detWpZRSf/jDH1RWVpaaP3+++vjjj9U555yjCgsL1f79+2O88h927bXXqszMTPXuu++qnTt3Nn/U19c351xzzTWqoKBAvfPOO2rVqlVq2LBhatiwYTFcdfvDOnF3nSjFWnED1gnrhMJjnbBO3MK1mxOllHrkkUdUQUGBSkpKUkOGDFErVqyI9ZIcW7RokQKgfYwdO1YpdWCs3R133KGys7OVz+dTp5xyitqwYUNsF22D9JgAqJkzZzbn7N+/X1133XWqc+fOKjU1VZ133nlq586dsVt0O8U6cTfWijuwTtyNdeIOrBN36yh1YimlVOTfjyEiIiIiInLGlT0nRERERETU8XBzQkRERERErsDNCRERERERuQI3J0RERERE5ArcnBARERERkStwc0JERERERK7AzQkREREREbkCNydEREREROQK3JwQEREREZErcHNCRERERESuwM0JERERERG5AjcnRERERETkCv8PPTd58aBDyxcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indeces = [0, 1, 4, 8]\n",
    "for i, ind in enumerate(indeces):\n",
    "    plt.subplot(1, len(indeces), i + 1)\n",
    "    plt.imshow(X_train[ind].reshape([28, 28]))\n",
    "    plt.title(f'$y_{ind}$=' + str(y_train[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 1),  # на вход картинки 28 на 28 -- ровно 784 значения\n",
    "    nn.Sigmoid(),  # на выход числа от 0 до 1, где 1 -- вероятность того, что на картинке B\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на параметры модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shapes: [torch.Size([1, 784]), torch.Size([1])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight shapes:\", [w.shape for w in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4420, 0.5629, 0.5607], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тензор размера 3 на 784 -- ровно три первых картинки из тренировочной выборки\n",
    "x = torch.tensor(X_train[:3], dtype=torch.float32)\n",
    "\n",
    "# тензор размера 3 -- ровно три первых ответа из тренировочной выборки\n",
    "y = torch.tensor(y_train[:3], dtype=torch.float32)\n",
    "\n",
    "# то, что предсказывает необученная модель\n",
    "y_predicted = model(x)[:, 0]\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения необходимо вычислить функцию ошибки. В данном случае подходит лог-лосс, или бинарная кросс-энтропия. Вы реализовали ровно в одной из функций из начала этой тетрадки :) Воспользуйтесь ей, чтобы посчитать лосс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8222, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <YOUR-CODE-HERE>\n",
    "def log_loss(predicted_probas, y_true): \n",
    "    return -torch.mean(torch.log(torch.abs(predicted_probas-(1-y_true))))\n",
    "\n",
    "loss = log_loss(y_predicted, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tuple(loss.size()) == tuple(), \"Лосс должен быть скаляром\"\n",
    "assert loss.data.numpy() > 0, \"лог лосс должен быть больше нуля, ноль -- только для идеального предсказания\"\n",
    "assert loss.data.numpy() <= np.log(3), \"Лосс слишком большой для нетренированной модели. Пожалуйста, перепроверьте его\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.05)  # создаем оптимизатор\n",
    "#  теперь как им пользоваться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(y_predicted, y)  # посчитали значение (скаляр), которое хотим минимизировать\n",
    "loss.backward()  # теперь у каждой переменной, где require_grad=True, есть градиент (производная). Для всех параметров нашего модуля это выполнено\n",
    "optimizer.step()  # делаем шаг градиентного спуска\n",
    "optimizer.zero_grad()  # очищаем параметры от уже посчитанных градиентов, чтобы те не мешали нам на следующих шагах\n",
    "\n",
    "# и да, это надо делать в цикле, который называется training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs: np.ndarray, targets: np.ndarray, batch_size: int, shuffle: bool = False) \\\n",
    "        -> Iterator[Tuple[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Проходимся по датасету мини-батчами \n",
    "    \"\"\"\n",
    "    assert len(inputs) == len(targets)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    \n",
    "    for start_idx in range(0, len(inputs), batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx: start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        \n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь всё вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128), # 784*64 + 64\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128,128), # 64*32 + 32\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(128,64), # 64*32 + 32\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(64,1), # 32*1 + 1\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "optimizer = Adam(model.parameters(), lr=0.000075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5a1ea8d2b24e21bd10f039276d556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(120)):    \n",
    "    history = []\n",
    "\n",
    "    # итерируемся по мини-батчам в цикле\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, 64):\n",
    "\n",
    "        # предскажи вероятности\n",
    "        y_predicted = model(torch.tensor(X_batch))[:,0]\n",
    "\n",
    "        assert y_predicted.dim() == 1, \"вы забыли второе измерение с помощью [:, 0]\"\n",
    "\n",
    "        # посчитай лосс\n",
    "        loss = log_loss(y_predicted, torch.tensor(y_batch))\n",
    "        history.append(loss.detach().numpy())\n",
    "\n",
    "        # посчитай градиенты\n",
    "        loss.backward()\n",
    "\n",
    "        # шаг градиентного спуска\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # чистим градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if i % 5 == 0:\n",
    "            # print(\"step #%i | mean loss = %.3f\" % (i, np.mean(history[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.3057136e-05, dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.97759\n"
     ]
    }
   ],
   "source": [
    "# посчитай качество на валидационной выборке\n",
    "def accuracy(predictions: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5):\n",
    "    \"\"\"точность предсказаний - доля верных ответов\"\"\"\n",
    "    ### <YOUR CODE HERE> ###\n",
    "    arr = (predictions>threshold) == targets\n",
    "    return torch.sum(arr)/arr.shape[0]\n",
    "\n",
    "predictions = torch.empty(len(X_test))\n",
    "for i, x in enumerate(X_test):\n",
    "    x_tensor = torch.tensor(x)\n",
    "    predictions[i] = model(x_tensor.reshape(1, len(x_tensor)))\n",
    "    \n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "acc = accuracy(predictions, y_test_tensor, 0.5)\n",
    "\n",
    "\n",
    "print(\"Test accuracy: %.5f\" % acc)\n",
    "assert acc > 0.94, \"попробуй потренировать подольше\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонус 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуй добиться того, чтобы accuracy на тестовой выборке стала больше чем 0.96 (для этого можно модицифировать архитектуру нейросети, менять параметры выше). Копипастить из этой же тетрадки или других можно :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Бонус 2. MLP + регрессия\n",
    "\n",
    "Попробуй построить нейросеть, которая решает задачу регрессии для датасета House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_predicted, y_true):\n",
    "    return torch.mean(torch.pow(y_predicted-y_true, 2))\n",
    "def rmse(y_pred, y_true):\n",
    "    return torch.square(torch.mean(torch.pow(y_true-y_pred, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "features = ['LotArea', 'TotalBsmtSF', 'GrLivArea', '1stFlrSF', '2ndFlrSF']\n",
    "data = pd.read_csv('house-prices-advanced-regression-techniques/train.csv')[features + ['SalePrice']]\n",
    "\n",
    "# train, valid = train_test_split(data, test_size=0.33, random_state=42)\n",
    "\n",
    "# X_train = train[features].values\n",
    "# y_train = train['SalePrice'].values[..., np.newaxis]\n",
    "\n",
    "# X_valid = valid[features].values\n",
    "# y_valid = valid['SalePrice'].values[..., np.newaxis]\n",
    "x_train_ind = list(range(len(data)))\n",
    "X_train, y = data.drop(columns=['SalePrice']).values, data['SalePrice'].values[..., np.newaxis]\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(x_train_ind)\n",
    "X_train = X_train[x_train_ind]\n",
    "y_train = y[x_train_ind]\n",
    "\n",
    "num_train_train = int(len(X_train)*0.8)\n",
    "\n",
    "x_train_train = X_train[:num_train_train]\n",
    "y_train_train = y_train[:num_train_train]\n",
    "\n",
    "y_train_test = y_train[int(len(X_train)*0.8):]\n",
    "x_train_test = X_train[int(len(X_train)*0.8):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(x_train_train)\n",
    "X_valid = scaler.transform(x_train_test)\n",
    "\n",
    "y_train_std = y_train_train.std()\n",
    "y_train = y_train_train / y_train_std\n",
    "y_valid = y_train_test / y_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 5)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 1),\n",
    "    \n",
    ")\n",
    "optimizer = SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2abb6845d284732bb01f898f3d35d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(600)):    \n",
    "    history = []\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, 64):\n",
    "    # for X, y in zip(X_train, y_train):\n",
    "        y_predicted = model(torch.tensor(X_batch).to(torch.float32))\n",
    "        loss = mse_loss(y_predicted, torch.tensor(y_batch))\n",
    "        history.append(loss.detach().numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28f4e0550>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(history)), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(torch.tensor(X_valid).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1405, dtype=torch.float64, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(rmse(y_pred, torch.tensor(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7765],\n",
       "        [4.0561],\n",
       "        [1.6941],\n",
       "        [2.2313],\n",
       "        [3.1796],\n",
       "        [1.3999],\n",
       "        [2.6036],\n",
       "        [1.8610],\n",
       "        [1.4016],\n",
       "        [2.0815],\n",
       "        [1.9816],\n",
       "        [1.5708],\n",
       "        [1.7059],\n",
       "        [2.9605],\n",
       "        [2.3482],\n",
       "        [1.6855],\n",
       "        [2.4536],\n",
       "        [1.7756],\n",
       "        [1.6922],\n",
       "        [2.7958],\n",
       "        [2.5208],\n",
       "        [2.2605],\n",
       "        [2.0954],\n",
       "        [1.5874],\n",
       "        [2.6990],\n",
       "        [2.0579],\n",
       "        [2.4486],\n",
       "        [1.6887],\n",
       "        [2.3114],\n",
       "        [2.4667],\n",
       "        [1.4055],\n",
       "        [2.9943],\n",
       "        [4.1904],\n",
       "        [1.6119],\n",
       "        [3.1841],\n",
       "        [1.8957],\n",
       "        [2.7118],\n",
       "        [2.7884],\n",
       "        [3.8870],\n",
       "        [1.3815],\n",
       "        [1.4238],\n",
       "        [2.7432],\n",
       "        [1.6276],\n",
       "        [3.8383],\n",
       "        [1.6788],\n",
       "        [3.0425],\n",
       "        [1.5848],\n",
       "        [1.6093],\n",
       "        [3.8157],\n",
       "        [2.2091],\n",
       "        [1.5756],\n",
       "        [3.0753],\n",
       "        [1.4765],\n",
       "        [3.7590],\n",
       "        [1.5281],\n",
       "        [3.3820],\n",
       "        [2.2769],\n",
       "        [2.0097],\n",
       "        [1.7790],\n",
       "        [1.3885],\n",
       "        [1.3991],\n",
       "        [2.1858],\n",
       "        [3.6305],\n",
       "        [2.9012],\n",
       "        [3.3360],\n",
       "        [2.9398],\n",
       "        [1.5481],\n",
       "        [3.5558],\n",
       "        [1.6750],\n",
       "        [1.9239],\n",
       "        [1.3816],\n",
       "        [1.7500],\n",
       "        [1.5867],\n",
       "        [1.3816],\n",
       "        [4.1434],\n",
       "        [2.2947],\n",
       "        [3.5573],\n",
       "        [3.5573],\n",
       "        [1.9296],\n",
       "        [1.5051],\n",
       "        [1.4227],\n",
       "        [1.6729],\n",
       "        [1.8244],\n",
       "        [1.3985],\n",
       "        [1.9688],\n",
       "        [2.0189],\n",
       "        [2.8066],\n",
       "        [2.7475],\n",
       "        [1.5281],\n",
       "        [2.1318],\n",
       "        [3.0478],\n",
       "        [2.3758],\n",
       "        [1.5326],\n",
       "        [4.1744],\n",
       "        [3.4462],\n",
       "        [2.3869],\n",
       "        [2.1176],\n",
       "        [1.9114],\n",
       "        [2.8831],\n",
       "        [3.2352],\n",
       "        [3.2479],\n",
       "        [2.5174],\n",
       "        [4.0024],\n",
       "        [1.5316],\n",
       "        [2.5920],\n",
       "        [2.8262],\n",
       "        [1.7187],\n",
       "        [3.4347],\n",
       "        [1.6687],\n",
       "        [3.0741],\n",
       "        [1.3986],\n",
       "        [1.7385],\n",
       "        [1.7509],\n",
       "        [2.0336],\n",
       "        [2.5786],\n",
       "        [1.5119],\n",
       "        [1.5679],\n",
       "        [1.5585],\n",
       "        [1.3991],\n",
       "        [3.9435],\n",
       "        [1.4594],\n",
       "        [1.5281],\n",
       "        [2.2523],\n",
       "        [2.5260],\n",
       "        [2.6235],\n",
       "        [1.6767],\n",
       "        [3.2747],\n",
       "        [1.4591],\n",
       "        [1.8301],\n",
       "        [2.4156],\n",
       "        [2.2505],\n",
       "        [3.6946],\n",
       "        [2.6002],\n",
       "        [2.3148],\n",
       "        [1.3990],\n",
       "        [3.3307],\n",
       "        [3.3501],\n",
       "        [1.6082],\n",
       "        [3.2739],\n",
       "        [6.1426],\n",
       "        [4.4257],\n",
       "        [1.6639],\n",
       "        [2.2401],\n",
       "        [2.3004],\n",
       "        [2.0766],\n",
       "        [1.5622],\n",
       "        [3.0365],\n",
       "        [2.6745],\n",
       "        [1.4982],\n",
       "        [1.5598],\n",
       "        [1.5480],\n",
       "        [1.8808],\n",
       "        [3.8667],\n",
       "        [3.3714],\n",
       "        [1.7274],\n",
       "        [2.5756],\n",
       "        [2.1544],\n",
       "        [2.5400],\n",
       "        [1.3990],\n",
       "        [1.6342],\n",
       "        [2.1641],\n",
       "        [1.7775],\n",
       "        [3.2424],\n",
       "        [2.4771],\n",
       "        [1.6923],\n",
       "        [3.5217],\n",
       "        [2.7679],\n",
       "        [3.9183],\n",
       "        [4.4876],\n",
       "        [3.4872],\n",
       "        [3.6594],\n",
       "        [1.5539],\n",
       "        [1.4885],\n",
       "        [2.0524],\n",
       "        [4.1629],\n",
       "        [1.3883],\n",
       "        [2.0408],\n",
       "        [2.4855],\n",
       "        [1.8510],\n",
       "        [2.0578],\n",
       "        [2.1725],\n",
       "        [1.4437],\n",
       "        [1.6140],\n",
       "        [1.7805],\n",
       "        [2.8816],\n",
       "        [2.1696],\n",
       "        [3.1597],\n",
       "        [2.6491],\n",
       "        [2.5193],\n",
       "        [1.4538],\n",
       "        [1.7361],\n",
       "        [1.8192],\n",
       "        [1.6329],\n",
       "        [1.8357],\n",
       "        [2.3439],\n",
       "        [2.6329],\n",
       "        [2.1752],\n",
       "        [1.8578],\n",
       "        [2.2886],\n",
       "        [1.7481],\n",
       "        [3.2176],\n",
       "        [2.3968],\n",
       "        [1.9540],\n",
       "        [3.9757],\n",
       "        [2.4389],\n",
       "        [1.6652],\n",
       "        [2.8882],\n",
       "        [1.8732],\n",
       "        [1.9226],\n",
       "        [1.5710],\n",
       "        [2.9705],\n",
       "        [1.5261],\n",
       "        [1.7479],\n",
       "        [1.9293],\n",
       "        [2.8140],\n",
       "        [3.0700],\n",
       "        [2.3794],\n",
       "        [1.9177],\n",
       "        [2.0248],\n",
       "        [1.3877],\n",
       "        [1.5961],\n",
       "        [2.3961],\n",
       "        [2.9217],\n",
       "        [1.5374],\n",
       "        [2.7949],\n",
       "        [2.1206],\n",
       "        [1.5666],\n",
       "        [1.4972],\n",
       "        [2.3453],\n",
       "        [1.6832],\n",
       "        [1.3816],\n",
       "        [2.1218],\n",
       "        [1.5543],\n",
       "        [2.1295],\n",
       "        [2.7770],\n",
       "        [1.9790],\n",
       "        [2.0790],\n",
       "        [1.6212],\n",
       "        [3.0591],\n",
       "        [1.5924],\n",
       "        [1.4360],\n",
       "        [2.9098],\n",
       "        [2.4609],\n",
       "        [4.5522],\n",
       "        [2.6631],\n",
       "        [2.0987],\n",
       "        [2.1838],\n",
       "        [1.9490],\n",
       "        [1.7849],\n",
       "        [1.5031],\n",
       "        [3.0388],\n",
       "        [1.7516],\n",
       "        [1.9898],\n",
       "        [1.4430],\n",
       "        [1.8296],\n",
       "        [1.8234],\n",
       "        [1.6149],\n",
       "        [1.6851],\n",
       "        [2.1965],\n",
       "        [2.9985],\n",
       "        [3.6774],\n",
       "        [2.0933],\n",
       "        [1.5915],\n",
       "        [3.0786],\n",
       "        [3.3857],\n",
       "        [2.9551],\n",
       "        [2.2710],\n",
       "        [1.5281],\n",
       "        [1.7090],\n",
       "        [1.6533],\n",
       "        [3.8489],\n",
       "        [2.4292],\n",
       "        [2.8255],\n",
       "        [1.4671],\n",
       "        [1.3815],\n",
       "        [1.6748],\n",
       "        [2.2538],\n",
       "        [3.6397],\n",
       "        [2.8234],\n",
       "        [2.0792],\n",
       "        [2.6729],\n",
       "        [1.4000],\n",
       "        [2.2259],\n",
       "        [1.5124],\n",
       "        [3.3510],\n",
       "        [2.8588],\n",
       "        [2.7322],\n",
       "        [1.6707],\n",
       "        [2.9519],\n",
       "        [2.0540],\n",
       "        [2.1581],\n",
       "        [1.5591],\n",
       "        [2.3755],\n",
       "        [2.2119],\n",
       "        [1.5238],\n",
       "        [2.2306],\n",
       "        [2.2691],\n",
       "        [1.5864],\n",
       "        [2.3343],\n",
       "        [1.5811],\n",
       "        [2.6841],\n",
       "        [2.8781],\n",
       "        [1.5452],\n",
       "        [1.8108],\n",
       "        [2.6197],\n",
       "        [2.5367],\n",
       "        [1.9672],\n",
       "        [3.2912],\n",
       "        [3.5575],\n",
       "        [1.5373],\n",
       "        [1.9993],\n",
       "        [3.1953],\n",
       "        [1.5214],\n",
       "        [3.8253],\n",
       "        [2.1112],\n",
       "        [2.1392],\n",
       "        [2.0303],\n",
       "        [1.6681],\n",
       "        [3.5364],\n",
       "        [1.3816],\n",
       "        [2.8958],\n",
       "        [2.1397],\n",
       "        [1.5320],\n",
       "        [1.5441],\n",
       "        [3.3117],\n",
       "        [2.6839],\n",
       "        [1.5504],\n",
       "        [2.3236],\n",
       "        [2.9124],\n",
       "        [1.5665],\n",
       "        [1.9134],\n",
       "        [2.5807],\n",
       "        [1.5223],\n",
       "        [2.4537],\n",
       "        [1.8244],\n",
       "        [1.7773],\n",
       "        [1.6805],\n",
       "        [1.9451],\n",
       "        [1.5802],\n",
       "        [2.6991],\n",
       "        [2.3646],\n",
       "        [1.5535],\n",
       "        [2.4851],\n",
       "        [2.6497],\n",
       "        [2.8309],\n",
       "        [2.9347],\n",
       "        [3.4785],\n",
       "        [1.6279],\n",
       "        [3.4713],\n",
       "        [1.4893],\n",
       "        [3.9890],\n",
       "        [4.1241],\n",
       "        [3.1926],\n",
       "        [2.3405],\n",
       "        [3.3234],\n",
       "        [1.6921],\n",
       "        [1.6334],\n",
       "        [1.4965],\n",
       "        [2.3661],\n",
       "        [3.9536],\n",
       "        [2.7300],\n",
       "        [1.7657],\n",
       "        [3.1241],\n",
       "        [2.8791],\n",
       "        [1.9109],\n",
       "        [2.6356],\n",
       "        [1.9495],\n",
       "        [1.6541],\n",
       "        [1.9546],\n",
       "        [2.7316],\n",
       "        [1.7229],\n",
       "        [2.3287],\n",
       "        [1.3943],\n",
       "        [2.0145],\n",
       "        [2.5035],\n",
       "        [1.5947],\n",
       "        [3.3956],\n",
       "        [3.5642],\n",
       "        [1.4227],\n",
       "        [3.1804],\n",
       "        [1.8048],\n",
       "        [2.6914],\n",
       "        [2.2038],\n",
       "        [2.0257],\n",
       "        [2.3179],\n",
       "        [2.4675],\n",
       "        [1.8668],\n",
       "        [2.8527],\n",
       "        [3.9239],\n",
       "        [2.2887],\n",
       "        [1.5929],\n",
       "        [1.3810],\n",
       "        [1.7575],\n",
       "        [3.5337],\n",
       "        [2.0915],\n",
       "        [4.2906],\n",
       "        [1.7561],\n",
       "        [2.4457],\n",
       "        [2.0713],\n",
       "        [1.3975],\n",
       "        [2.0495],\n",
       "        [4.0373],\n",
       "        [2.7287],\n",
       "        [1.8285],\n",
       "        [1.8533],\n",
       "        [1.5733],\n",
       "        [2.4872],\n",
       "        [3.1004],\n",
       "        [4.0486],\n",
       "        [2.3647],\n",
       "        [2.4834],\n",
       "        [2.8256],\n",
       "        [1.3989],\n",
       "        [3.3623],\n",
       "        [2.2504],\n",
       "        [3.7432],\n",
       "        [1.5677],\n",
       "        [3.1862],\n",
       "        [3.1897],\n",
       "        [1.7668],\n",
       "        [2.9871],\n",
       "        [2.8738],\n",
       "        [2.3649],\n",
       "        [2.2107],\n",
       "        [3.4062],\n",
       "        [2.2061],\n",
       "        [2.4900],\n",
       "        [1.7032],\n",
       "        [2.1171],\n",
       "        [3.0719],\n",
       "        [1.5600],\n",
       "        [1.7074],\n",
       "        [2.8347],\n",
       "        [1.7678],\n",
       "        [1.9679],\n",
       "        [2.3576],\n",
       "        [1.7550],\n",
       "        [4.9340],\n",
       "        [2.7687],\n",
       "        [1.4254],\n",
       "        [2.1964],\n",
       "        [3.6627],\n",
       "        [2.3999],\n",
       "        [2.3180],\n",
       "        [2.6779],\n",
       "        [1.8927],\n",
       "        [2.2617],\n",
       "        [2.8967],\n",
       "        [1.8456],\n",
       "        [1.7583],\n",
       "        [1.5835],\n",
       "        [2.4595],\n",
       "        [2.8717],\n",
       "        [2.5729],\n",
       "        [1.7178],\n",
       "        [2.2669],\n",
       "        [3.7151],\n",
       "        [3.3004],\n",
       "        [2.9135],\n",
       "        [2.4261],\n",
       "        [1.7306],\n",
       "        [1.3989],\n",
       "        [3.3616],\n",
       "        [3.1344],\n",
       "        [2.5993],\n",
       "        [6.4530],\n",
       "        [2.5388],\n",
       "        [2.6356],\n",
       "        [2.2748],\n",
       "        [3.4908],\n",
       "        [1.5537],\n",
       "        [1.3817],\n",
       "        [1.5929],\n",
       "        [1.5353],\n",
       "        [2.7469],\n",
       "        [3.9363],\n",
       "        [1.7732],\n",
       "        [2.1062],\n",
       "        [1.7721],\n",
       "        [2.6149],\n",
       "        [1.6235],\n",
       "        [2.4788]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b57e1ee3ce2d9880d8a1431cc6218bd6dd8233a03980e7e673f351183fa295a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
